2025-05-13 12:33:51,445 - deepseek_api - INFO - ÕıÔÚ´Ó±¾µØ¼ÓÔØDeepSeekÄ£ĞÍ: models/deepseek-coder-1.3b-base...
2025-05-13 12:33:51,446 - deepseek_api - INFO - GPU: NVIDIA GeForce GTX 1650 (4GB)
2025-05-13 12:33:51,447 - deepseek_api - INFO - ÕıÔÚ¼ÓÔØ·Ö´ÊÆ÷...
2025-05-13 12:33:51,597 - deepseek_api - INFO - ·Ö´ÊÆ÷¼ÓÔØÍê³É
2025-05-13 12:33:51,598 - deepseek_api - INFO - ÇåÀíGPU»º´æ...
2025-05-13 12:33:51,598 - deepseek_api - INFO - ³õÊ¼GPUÄÚ´æÊ¹ÓÃ: 0.00GB
2025-05-13 12:33:51,674 - deepseek_api - INFO - ÆôÓÃ8Î»Á¿»¯ÒÔ½ÚÊ¡GPUÄÚ´æ
2025-05-13 12:33:51,675 - deepseek_api - INFO - ÕıÔÚ¼ÓÔØÄ£ĞÍ£¨Ê¹ÓÃÓÅ»¯ÉèÖÃ£©...
2025-05-13 12:33:51,680 - deepseek_api - INFO - ¹À¼ÆÄ£ĞÍ´óĞ¡: 0.26GB£¨Î´¾­Á¿»¯£©
2025-05-13 12:33:55,235 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-05-13 12:34:02,555 - deepseek_api - INFO - Ä£ĞÍ¼ÓÔØºóGPUÄÚ´æÊ¹ÓÃ: 1.48GB (Ôö¼ÓÁË1.48GB)
2025-05-13 12:34:02,556 - deepseek_api - INFO - DeepSeekÄ£ĞÍ¼ÓÔØÍê³É
2025-05-13 12:34:02,557 - deepseek_api - INFO - ×îÖÕGPUÄÚ´æÊ¹ÓÃ: 1.48GB / 4GB
2025-05-13 12:34:02,558 - deepseek_api - INFO - ÒÑÆôÓÃKV»º´æÒÔÌá¸ßÍÆÀíËÙ¶È
2025-05-13 12:34:02,596 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.4.179.65:5000
2025-05-13 12:34:02,597 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-05-13 12:36:55,520 - deepseek_api - INFO - ÕıÔÚ´Ó±¾µØ¼ÓÔØDeepSeekÄ£ĞÍ: models/deepseek-coder-1.3b-base...
2025-05-13 12:36:55,520 - deepseek_api - INFO - GPU: NVIDIA GeForce GTX 1650 (4GB)
2025-05-13 12:36:55,521 - deepseek_api - INFO - ÕıÔÚ¼ÓÔØ·Ö´ÊÆ÷...
2025-05-13 12:36:55,624 - deepseek_api - INFO - ·Ö´ÊÆ÷¼ÓÔØÍê³É
2025-05-13 12:36:55,624 - deepseek_api - INFO - ÇåÀíGPU»º´æ...
2025-05-13 12:36:55,624 - deepseek_api - INFO - ³õÊ¼GPUÄÚ´æÊ¹ÓÃ: 0.00GB
2025-05-13 12:36:55,696 - deepseek_api - INFO - ÆôÓÃ8Î»Á¿»¯ÒÔ½ÚÊ¡GPUÄÚ´æ
2025-05-13 12:36:55,696 - deepseek_api - INFO - ÕıÔÚ¼ÓÔØÄ£ĞÍ£¨Ê¹ÓÃÓÅ»¯ÉèÖÃ£©...
2025-05-13 12:36:55,701 - deepseek_api - INFO - ¹À¼ÆÄ£ĞÍ´óĞ¡: 0.26GB£¨Î´¾­Á¿»¯£©
2025-05-13 12:36:58,330 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-05-13 12:37:02,803 - deepseek_api - INFO - Ä£ĞÍ¼ÓÔØºóGPUÄÚ´æÊ¹ÓÃ: 1.48GB (Ôö¼ÓÁË1.48GB)
2025-05-13 12:37:02,804 - deepseek_api - INFO - DeepSeekÄ£ĞÍ¼ÓÔØÍê³É
2025-05-13 12:37:02,804 - deepseek_api - INFO - ×îÖÕGPUÄÚ´æÊ¹ÓÃ: 1.48GB / 4GB
2025-05-13 12:37:02,804 - deepseek_api - INFO - ÒÑÆôÓÃKV»º´æÒÔÌá¸ßÍÆÀíËÙ¶È
2025-05-13 12:37:02,824 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.4.179.65:5000
2025-05-13 12:37:02,824 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-05-13 12:37:09,168 - werkzeug - INFO - 127.0.0.1 - - [13/May/2025 12:37:09] "GET /health HTTP/1.1" 200 -
2025-05-13 12:38:00,753 - deepseek_api - INFO - ÊÕµ½²éÑ¯ (³¤¶È: 2×Ö·û)
2025-05-13 12:38:00,781 - deepseek_api - INFO - Éú³ÉÇ°GPUÄÚ´æ: 1.48GB
2025-05-13 12:38:00,798 - deepseek_api - INFO - ÊäÈë³¤¶È: 12 tokens
2025-05-13 12:38:00,801 - deepseek_api - INFO - ×î´óĞÂtokenÊıÁ¿: 256
2025-05-13 12:38:00,801 - deepseek_api - INFO - ¿ªÊ¼Éú³É»Ø´ğ...
2025-05-13 12:38:00,806 - deepseek_api - ERROR - Éú³É»Ø´ğÊ§°Ü: transformers.generation.utils.GenerationMixin.generate() got multiple values for keyword argument 'max_new_tokens'
Traceback (most recent call last):
  File "D:\bs\PEPPER_AI_Classroom\simple_api_server.py", line 224, in llm_query_api
    output = model.generate(
TypeError: transformers.generation.utils.GenerationMixin.generate() got multiple values for keyword argument 'max_new_tokens'
2025-05-13 12:38:00,807 - werkzeug - INFO - 127.0.0.1 - - [13/May/2025 12:38:00] "[35m[1mPOST /llm/query HTTP/1.1[0m" 500 -
