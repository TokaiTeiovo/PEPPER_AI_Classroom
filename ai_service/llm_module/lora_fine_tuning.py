"""
LoRAå¾®è°ƒæ¨¡å— - ç”¨äºPEPPERæœºå™¨äººæ•™å­¦ç³»ç»Ÿçš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒ
"""

import logging
import os
from datetime import datetime

import torch
from datasets import Dataset
from peft import (
    get_peft_model,
    LoraConfig,
    TaskType,
    prepare_model_for_kbit_training
)
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("LORA_FINE_TUNING")


class LoRAFineTuner:
    def __init__(
            self,
            base_model_path="models/deepseek-coder-1.3b-base",
            output_dir=None,
            lora_r=16,
            lora_alpha=32,
            lora_dropout=0.05
    ):
        """åˆå§‹åŒ–LoRAå¾®è°ƒå™¨"""
        self.base_model_path = base_model_path

        # åªæœ‰æ˜ç¡®æŒ‡å®šoutput_diræ—¶æ‰åˆ›å»ºç›®å½•
        if output_dir is not None:
            self.output_dir = output_dir
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(self.output_dir, exist_ok=True)
            logger.info(f"æ¨¡å‹å°†ä¿å­˜åˆ°: {self.output_dir}")
        else:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šoutput_dirï¼Œä¸åˆ›å»ºä»»ä½•ç›®å½•
            self.output_dir = None
            logger.info("æ¨ç†æ¨¡å¼ï¼Œä¸åˆ›å»ºè¾“å‡ºç›®å½•")

        self.lora_r = lora_r
        self.lora_alpha = lora_alpha
        self.lora_dropout = lora_dropout
        self.tokenizer = None
        self.model = None
        self.peft_model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"


    def load_base_model(self, use_4bit=True, use_8bit=False):
        """åŠ è½½åŸºç¡€æ¨¡å‹"""
        try:
            logger.info(f"æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: {self.base_model_path}")

            # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not os.path.exists(self.base_model_path):
                logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.base_model_path}")
                return False

            # åŠ è½½åˆ†è¯å™¨
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.base_model_path,
                    trust_remote_code=True,
                )
                logger.info("åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
                return False

            # è®¾ç½®åˆ†è¯å™¨çš„å¡«å……å’Œç‰¹æ®Štoken
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            # ç¡®å®šè®¾å¤‡å’Œæ•°æ®ç±»å‹
            device = "cuda" if torch.cuda.is_available() else "cpu"
            dtype = torch.float16 if torch.cuda.is_available() else torch.float32

            logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}, æ•°æ®ç±»å‹: {dtype}")

            # é…ç½®é‡åŒ–å‚æ•°
            quantization_config = None

            if device == "cuda" and (use_4bit or use_8bit):
                try:
                    if use_4bit:
                        logger.info("ä½¿ç”¨4bité‡åŒ–é…ç½®")
                        quantization_config = BitsAndBytesConfig(
                            load_in_4bit=True,
                            bnb_4bit_quant_type="nf4",  # ä½¿ç”¨NF4é‡åŒ–ç±»å‹
                            bnb_4bit_compute_dtype=torch.float16,  # è®¡ç®—æ—¶ä½¿ç”¨float16
                            bnb_4bit_use_double_quant=True,  # åŒé‡é‡åŒ–ï¼Œè¿›ä¸€æ­¥èŠ‚çœå†…å­˜
                        )
                    elif use_8bit:
                        logger.info("ä½¿ç”¨8bité‡åŒ–é…ç½®")
                        quantization_config = BitsAndBytesConfig(
                            load_in_8bit=True,
                        )
                except Exception as e:
                    logger.warning(f"é‡åŒ–é…ç½®å¤±è´¥ï¼Œå°†ä½¿ç”¨æ ‡å‡†åŠ è½½: {e}")
                    quantization_config = None

            # åŠ è½½æ¨¡å‹
            load_kwargs = {
                "trust_remote_code": True,
                "torch_dtype": dtype,
            }

            # æ·»åŠ é‡åŒ–é…ç½®
            if quantization_config is not None:
                load_kwargs["quantization_config"] = quantization_config
                load_kwargs["device_map"] = "auto"
            elif device == "cuda":
                load_kwargs["device_map"] = "auto"
                load_kwargs["torch_dtype"] = torch.float16
            else:
                load_kwargs["torch_dtype"] = torch.float32

            # åŠ è½½æ¨¡å‹
            try:
                logger.info("å¼€å§‹åŠ è½½æ¨¡å‹...")
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.base_model_path,
                    **load_kwargs
                )

                # å¦‚æœä½¿ç”¨CPUä¸”æ²¡æœ‰é‡åŒ–ï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
                if device == "cpu" and quantization_config is None:
                    self.model = self.model.to(device)

                logger.info("åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")

                # æ‰“å°æ¨¡å‹ä¿¡æ¯
                if hasattr(self.model, 'get_memory_footprint'):
                    try:
                        memory_mb = self.model.get_memory_footprint() / 1024 / 1024
                        logger.info(f"æ¨¡å‹æ˜¾å­˜å ç”¨: {memory_mb:.2f} MB")
                    except:
                        pass

                return True

            except Exception as e:
                logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                return False

        except Exception as e:
            logger.error(f"load_base_model æ‰§è¡Œå¤±è´¥: {e}")
            return False

    def prepare_lora_config(self):
        """å‡†å¤‡LoRAé…ç½®"""
        try:
            logger.info("é…ç½®LoRAå‚æ•°")

            # ä¸ºDeepSeekæ¨¡å‹è·å–æ­£ç¡®çš„ç›®æ ‡æ¨¡å—
            if hasattr(self.model, "config") and hasattr(self.model.config, "model_type"):
                model_type = self.model.config.model_type
                logger.info(f"æ£€æµ‹åˆ°æ¨¡å‹ç±»å‹: {model_type}")
                if model_type == "deepseek":
                    # DeepSeekæ¨¡å‹çš„ç‰¹å®šæ¨¡å—
                    target_modules = ["k_proj", "q_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
                elif model_type in ["llama", "mistral"]:
                    # LLaMA/Mistralç±»å‹æ¨¡å‹
                    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
                else:
                    # é€šç”¨ç›®æ ‡æ¨¡å—
                    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
            else:
                # é»˜è®¤ç›®æ ‡æ¨¡å—
                target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]

            logger.info(f"ç›®æ ‡æ¨¡å—: {target_modules}")

            # å®šä¹‰LoRAé…ç½®
            peft_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                inference_mode=False,
                r=self.lora_r,
                lora_alpha=self.lora_alpha,
                lora_dropout=self.lora_dropout,
                target_modules=target_modules,
                bias="none",  # ä¸è®­ç»ƒbias
                modules_to_save=None,  # ä¸ä¿å­˜é¢å¤–æ¨¡å—
            )

            # ä¸ºé‡åŒ–è®­ç»ƒå‡†å¤‡æ¨¡å‹
            if hasattr(self.model, "is_loaded_in_8bit") and self.model.is_loaded_in_8bit:
                logger.info("ä¸º8bité‡åŒ–è®­ç»ƒå‡†å¤‡æ¨¡å‹")
                self.model = prepare_model_for_kbit_training(self.model)
            elif hasattr(self.model, "is_loaded_in_4bit") and self.model.is_loaded_in_4bit:
                logger.info("ä¸º4bité‡åŒ–è®­ç»ƒå‡†å¤‡æ¨¡å‹")
                self.model = prepare_model_for_kbit_training(self.model)

            # åº”ç”¨LoRAé…ç½®
            self.peft_model = get_peft_model(self.model, peft_config)

            # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
            trainable_params = 0
            all_param = 0
            for _, param in self.peft_model.named_parameters():
                all_param += param.numel()
                if param.requires_grad:
                    trainable_params += param.numel()

            logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / æ€»å‚æ•°: {all_param:,} "
                        f"({100 * trainable_params / all_param:.2f}%)")

            return self.peft_model

        except Exception as e:
            logger.error(f"LoRAé…ç½®å¤±è´¥: {e}")
            return None

    def prepare_dataset(self, data_path=None, instruction_data=None):
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®é›† - ä¼˜åŒ–ä¸­æ–‡å¯¹è¯æ ¼å¼
        æ”¯æŒä» JSON æˆ– CSV è¯»å– [{instruction, input, output}] æ ¼å¼
        """
        try:
            if data_path and os.path.exists(data_path):
                if data_path.endswith('.json'):
                    import json
                    with open(data_path, 'r', encoding='utf-8') as f:
                        instruction_data = json.load(f)
                elif data_path.endswith('.csv'):
                    import pandas as pd
                    instruction_data = pd.read_csv(data_path).to_dict('records')

            if not instruction_data:
                # ä¸­æ–‡ç¤ºä¾‹æ•°æ®
                instruction_data = [
                    {
                        "instruction": "è§£é‡ŠPythonä¸­forå¾ªç¯å’Œwhileå¾ªç¯çš„åŒºåˆ«",
                        "input": "",
                        "output": "æ‚¨å¥½ï¼forå¾ªç¯ç”¨äºéå†å·²çŸ¥åºåˆ—ï¼Œwhileå¾ªç¯åŸºäºæ¡ä»¶åˆ¤æ–­ã€‚forå¾ªç¯æ¬¡æ•°ç¡®å®šï¼Œwhileå¾ªç¯æ¬¡æ•°ä¸ç¡®å®šã€‚forå¾ªç¯è¯­æ³•ç®€æ´ï¼Œwhileå¾ªç¯éœ€è¦æ‰‹åŠ¨ç®¡ç†å¾ªç¯å˜é‡ã€‚"
                    },
                    {
                        "instruction": "ä½ å¥½",
                        "input": "",
                        "output": "æ‚¨å¥½ï¼æˆ‘æ˜¯PEPPERæ™ºèƒ½æ•™å­¦åŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"
                    },
                    {
                        "instruction": "ä»‹ç»ä¸€ä¸‹è‡ªå·±",
                        "input": "",
                        "output": "æˆ‘æ˜¯PEPPERæ™ºèƒ½æ•™å­¦åŠ©æ‰‹ï¼ŒåŸºäºDeepSeekæ¨¡å‹ã€‚æˆ‘å¯ä»¥å¸®åŠ©æ‚¨å­¦ä¹ ç¼–ç¨‹ã€äººå·¥æ™ºèƒ½ã€æ•°å­¦ç­‰çŸ¥è¯†ï¼Œæä¾›ä¸ªæ€§åŒ–çš„å­¦ä¹ å»ºè®®ã€‚"
                    }
                ]

            logger.info(f"å‡†å¤‡è®­ç»ƒæ•°æ®é›†ï¼Œå…±{len(instruction_data)}æ¡è®°å½•")

            # è½¬æ¢ä¸ºä¸­æ–‡å¯¹è¯æ ¼å¼
            def format_instruction(example):
                instruction = example["instruction"]
                input_text = example.get("input", "")
                output = example["output"]

                # ä½¿ç”¨ä¸­æ–‡æ ¼å¼çš„å¯¹è¯æ¨¡æ¿
                if input_text.strip():
                    prompt = f"ç”¨æˆ·: {instruction}\nè¡¥å……ä¿¡æ¯: {input_text}\n\nåŠ©æ‰‹: "
                else:
                    prompt = f"ç”¨æˆ·: {instruction}\n\nåŠ©æ‰‹: "

                full_text = prompt + output
                return {"text": full_text}

            formatted_data = [format_instruction(item) for item in instruction_data]
            raw_dataset = Dataset.from_list(formatted_data)

            # åˆ†è¯ + æ·»åŠ  labelsï¼Œå¹¶åˆ é™¤åŸå§‹textå­—æ®µ
            def tokenize_function(examples):
                tokens = self.tokenizer(
                    examples["text"],
                    padding="max_length",
                    truncation=True,
                    max_length=1024
                )
                tokens["labels"] = tokens["input_ids"].copy()
                return tokens

            tokenized_dataset = raw_dataset.map(tokenize_function, batched=False)
            tokenized_dataset = tokenized_dataset.remove_columns(["text"])

            return tokenized_dataset

        except Exception as e:
            logger.error(f"æ•°æ®é›†å‡†å¤‡å¤±è´¥: {e}")
            return None

        def generate_response(self, prompt, max_new_tokens=512):
            """ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆå›ç­” - å¼ºåŒ–ä¸­æ–‡è¾“å‡º"""
            if not self.peft_model:
                logger.error("æ¨¡å‹æœªåŠ è½½")
                return "æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆå›ç­”"

            try:
                # ä¸ºDeepSeekæ¨¡å‹è°ƒæ•´ä¸­æ–‡æç¤ºæ ¼å¼
                if not prompt.startswith("ç”¨æˆ·:"):
                    system_prompt = "ä½ æ˜¯PEPPERæ™ºèƒ½æ•™å­¦åŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”æ‰€æœ‰é—®é¢˜ã€‚"
                    prompt = f"{system_prompt}\nç”¨æˆ·: {prompt}\n\nåŠ©æ‰‹: "

                # ç¼–ç è¾“å…¥
                inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

                # è®¾ç½®ç”Ÿæˆå‚æ•°ï¼Œä¼˜åŒ–ä¸­æ–‡ç”Ÿæˆ
                gen_kwargs = {
                    "max_new_tokens": min(max_new_tokens, 200),  # é™åˆ¶é•¿åº¦
                    "temperature": 0.8,
                    "top_p": 0.9,
                    "top_k": 40,
                    "do_sample": True,
                    "pad_token_id": self.tokenizer.eos_token_id,
                    "repetition_penalty": 1.2,
                    "no_repeat_ngram_size": 3,
                }

                # ç”Ÿæˆå›ç­”
                with torch.no_grad():
                    output_ids = self.peft_model.generate(**inputs, **gen_kwargs)

                # è§£ç è¾“å‡º
                full_output = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)

                # æå–å›ç­”éƒ¨åˆ†
                if "åŠ©æ‰‹:" in full_output:
                    response = full_output.split("åŠ©æ‰‹:")[-1].strip()
                else:
                    response = full_output[len(prompt):].strip()

                # é™åˆ¶å›ç­”é•¿åº¦ï¼Œé¿å…è¿‡é•¿è¾“å‡º
                if len(response) > 300:
                    response = response[:300] + "..."

                return response

            except Exception as e:
                logger.error(f"ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
                return "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚è¯·ç¨åå†è¯•ã€‚"

    def train(self, dataset, epochs=3, batch_size=4, learning_rate=2e-4, progress_callback=None):
        """è®­ç»ƒæ¨¡å‹"""
        try:
            logger.info("å¼€å§‹è®­ç»ƒè¿‡ç¨‹")

            # æ£€æŸ¥æ˜¯å¦æ”¶åˆ°äº†è¿›åº¦å›è°ƒ
            if progress_callback:
                logger.info("å·²æ¥æ”¶åˆ°è¿›åº¦å›è°ƒå‡½æ•°")
            else:
                logger.warning("æœªæ¥æ”¶åˆ°è¿›åº¦å›è°ƒå‡½æ•°")

            if not self.peft_model:
                logger.error("PEFTæ¨¡å‹æœªåˆå§‹åŒ–")
                return False

            if not self.peft_model:
                logger.error("PEFTæ¨¡å‹æœªåˆå§‹åŒ–")
                return False

            # æ ¹æ®æ˜¯å¦ä½¿ç”¨é‡åŒ–è°ƒæ•´è®­ç»ƒå‚æ•°
            if hasattr(self.model, "is_loaded_in_4bit") and self.model.is_loaded_in_4bit:
                logger.info("ä½¿ç”¨4bité‡åŒ–è®­ç»ƒé…ç½®")
                # 4bitè®­ç»ƒå»ºè®®ä½¿ç”¨æ›´å°çš„batch sizeå’Œæ›´å¤§çš„gradient accumulation
                if batch_size > 2:
                    batch_size = 2
                    logger.info("4bitè®­ç»ƒå»ºè®®batch_sizeè®¾ç½®ä¸º2")
                gradient_accumulation_steps = 2  # å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
            elif hasattr(self.model, "is_loaded_in_8bit") and self.model.is_loaded_in_8bit:
                logger.info("ä½¿ç”¨8bité‡åŒ–è®­ç»ƒé…ç½®")
                gradient_accumulation_steps = 2
            else:
                gradient_accumulation_steps = 2

            # è®¡ç®—å®é™…è®­ç»ƒæ­¥æ•°å¹¶æ‰“å°è°ƒè¯•ä¿¡æ¯
            dataset_size = len(dataset)
            steps_per_epoch = max(1, dataset_size // (batch_size * gradient_accumulation_steps))
            total_steps = steps_per_epoch * epochs

            logger.info(f"ğŸ”è®­ç»ƒå‚æ•°è°ƒè¯•ä¿¡æ¯:")
            logger.info(f"æ•°æ®é›†å¤§å°: {dataset_size}")
            logger.info(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
            logger.info(f"æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
            logger.info(f"æ¯è½®æ­¥æ•°: {steps_per_epoch}")
            logger.info(f"æ€»è½®æ•°: {epochs}")
            logger.info(f"æ€»è®­ç»ƒæ­¥æ•°: {total_steps}")

            # å®šä¹‰è®­ç»ƒå‚æ•°
            training_args = TrainingArguments(
                output_dir=self.output_dir,
                num_train_epochs=epochs,
                per_device_train_batch_size=batch_size,
                gradient_accumulation_steps=gradient_accumulation_steps,
                learning_rate=learning_rate,
                weight_decay=0.01,
                warmup_steps=min(50, total_steps // 10),  # åŠ¨æ€è°ƒæ•´warmupæ­¥æ•°
                logging_steps=1,  # æ¯æ­¥éƒ½è®°å½•æ—¥å¿—
                logging_strategy="steps",
                save_steps=max(10, total_steps // 4),  # åŠ¨æ€è°ƒæ•´ä¿å­˜é¢‘ç‡
                save_total_limit=2,
                eval_strategy="no",  # å…³é—­è¯„ä¼°ä»¥åŠ å¿«è®­ç»ƒ
                fp16=(self.device == "cuda"),
                bf16=False,
                report_to="none",
                remove_unused_columns=False,
                dataloader_pin_memory=False,
                gradient_checkpointing=True,
                optim="adamw_torch",
                disable_tqdm=False,  # å¯ç”¨tqdmè¿›åº¦æ¡
                load_best_model_at_end=False,  # ä¸åŠ è½½æœ€ä½³æ¨¡å‹ä»¥èŠ‚çœæ—¶é—´
                max_steps=total_steps if total_steps > 0 else -1,  # è®¾ç½®æœ€å¤§æ­¥æ•°
            )

            # åˆ›å»ºæ•°æ®æ”¶é›†å™¨
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False  # ä¸ä½¿ç”¨æ©ç è¯­è¨€æ¨¡å‹è®­ç»ƒ
            )

            # è‡ªå®šä¹‰è¿›åº¦å›è°ƒç±»
            from transformers import TrainerCallback

            class ProgressCallback(TrainerCallback):
                def __init__(self, callback_func, total_epochs):
                    self.callback_func = callback_func
                    self.total_epochs = total_epochs
                    self.last_reported_progress = 0
                    self.current_epoch = 0
                    self.step_count = 0
                    logger.info(f"ProgressCallback åˆå§‹åŒ–å®Œæˆï¼Œæ€»è½®æ•°: {total_epochs}")

                def on_train_begin(self, args, state, control, **kwargs):
                    logger.info(f"è®­ç»ƒå¼€å§‹ - æ€»æ­¥æ•°: {state.max_steps}, æ€»è½®æ•°: {self.total_epochs}")
                    if self.callback_func:
                        self.callback_func(15)  # å¼€å§‹æ—¶è®¾ç½®ä¸º15%ï¼ˆæ¨¡å‹åŠ è½½å®Œæˆï¼‰

                def on_step_end(self, args, state, control, **kwargs):
                    self.step_count += 1
                    if self.callback_func and state.max_steps > 0:
                        # è®¡ç®—å½“å‰è¿›åº¦ï¼š15% + (å½“å‰æ­¥æ•°/æ€»æ­¥æ•°) * 80%ï¼ˆè®­ç»ƒå 80%è¿›åº¦ï¼‰
                        step_progress = (state.global_step / state.max_steps) * 80
                        total_progress = 15 + step_progress

                        # æ¯æ­¥éƒ½æŠ¥å‘Šè¿›åº¦
                        logger.info(f"ğŸ“Š è®­ç»ƒæ­¥éª¤: {state.global_step}/{state.max_steps}, è¿›åº¦: {total_progress:.1f}%")
                        self.callback_func(min(99, int(total_progress)))
                        self.last_reported_progress = total_progress

                def on_epoch_begin(self, args, state, control, **kwargs):
                    self.current_epoch = int(state.epoch) + 1
                    logger.info(f"ğŸ”„ å¼€å§‹ç¬¬ {self.current_epoch}/{self.total_epochs} è½®è®­ç»ƒ")

                def on_epoch_end(self, args, state, control, **kwargs):
                    # è½®æ¬¡ç»“æŸæ—¶å¼ºåˆ¶æ›´æ–°è¿›åº¦
                    if self.callback_func and state.max_steps > 0:
                        epoch_progress = (self.current_epoch / self.total_epochs) * 80
                        total_progress = 15 + epoch_progress
                        logger.info(f"âœ… ç¬¬ {self.current_epoch} è½®è®­ç»ƒå®Œæˆï¼Œè¿›åº¦: {total_progress:.1f}%")
                        self.callback_func(min(99, int(total_progress)))
                        self.last_reported_progress = total_progress

                def on_log(self, args, state, control, logs=None, **kwargs):
                    # åœ¨æ¯æ¬¡æ—¥å¿—è®°å½•æ—¶ä¹Ÿæ›´æ–°è¿›åº¦å’Œæ˜¾ç¤ºæŸå¤±
                    if self.callback_func and state.max_steps > 0 and logs:
                        step_progress = (state.global_step / state.max_steps) * 80
                        total_progress = 15 + step_progress

                        # æ˜¾ç¤ºè®­ç»ƒæŸå¤±ç­‰ä¿¡æ¯
                        if 'train_loss' in logs:
                            logger.info(
                                f"ğŸ“ˆ æ­¥éª¤ {state.global_step}: æŸå¤± = {logs['train_loss']:.4f}, è¿›åº¦ = {total_progress:.1f}%")

                        self.callback_func(min(99, int(total_progress)))

                def on_train_end(self, args, state, control, **kwargs):
                    logger.info("ğŸ‰ è®­ç»ƒç»“æŸ")
                    if self.callback_func:
                        self.callback_func(99)  # è®­ç»ƒç»“æŸæ—¶è®¾ç½®ä¸º99%ï¼ˆä¿å­˜æ¨¡å‹éœ€è¦æ—¶é—´ï¼‰

            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = Trainer(
                model=self.peft_model,
                args=training_args,
                train_dataset=dataset,
                data_collator=data_collator,
                tokenizer=self.tokenizer
            )

            # å¦‚æœæœ‰è¿›åº¦å›è°ƒï¼Œæ·»åŠ å›è°ƒ
            if progress_callback:
                logger.info("æ·»åŠ è¿›åº¦å›è°ƒåˆ°è®­ç»ƒå™¨")
                progress_cb = ProgressCallback(progress_callback, epochs)  # ä¼ å…¥æ€»è½®æ•°
                trainer.add_callback(progress_cb)
            else:
                logger.warning("æ²¡æœ‰è¿›åº¦å›è°ƒå‡½æ•°ï¼Œå°†æ— æ³•æ›´æ–°è®­ç»ƒè¿›åº¦")

            # å¼€å§‹è®­ç»ƒ
            logger.info("å¼€å§‹LoRAå¾®è°ƒè®­ç»ƒ")
            trainer.train()

            # ä¿å­˜æ¨¡å‹
            trainer.save_model()
            self.tokenizer.save_pretrained(self.output_dir)

            # ä¿å­˜è®­ç»ƒä¿¡æ¯
            training_info = {
                "base_model": self.base_model_path,
                "training_time": datetime.now().isoformat(),
                "epochs": epochs,
                "batch_size": batch_size,
                "learning_rate": learning_rate,
                "lora_r": self.lora_r,
                "lora_alpha": self.lora_alpha,
                "lora_dropout": self.lora_dropout,
                "output_dir": self.output_dir,
                "quantization": "4bit" if hasattr(self.model, "is_loaded_in_4bit") and self.model.is_loaded_in_4bit else
                "8bit" if hasattr(self.model, "is_loaded_in_8bit") and self.model.is_loaded_in_8bit else "none",
                "gradient_accumulation_steps": gradient_accumulation_steps,
                "total_trainable_params": sum(p.numel() for p in self.peft_model.parameters() if p.requires_grad)
            }

            info_file = os.path.join(self.output_dir, "training_info.json")
            import json
            try:
                with open(info_file, 'w', encoding='utf-8') as f:
                    json.dump(training_info, f, ensure_ascii=False, indent=2)
                logger.info(f"è®­ç»ƒä¿¡æ¯å·²ä¿å­˜åˆ°: {info_file}")
            except Exception as e:
                logger.warning(f"ä¿å­˜è®­ç»ƒä¿¡æ¯å¤±è´¥: {e}")

            logger.info("LoRAå¾®è°ƒè®­ç»ƒå®Œæˆ")
            return True

        except Exception as e:
            logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return False

    def generate_response(self, prompt, max_new_tokens=512):
        """ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆå›ç­”"""
        if not self.peft_model:
            logger.error("æ¨¡å‹æœªåŠ è½½")
            return "æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆå›ç­”"

        try:
            # ä¸ºDeepSeekæ¨¡å‹è°ƒæ•´æç¤ºæ ¼å¼ - ç®€åŒ–æ ¼å¼
            if not prompt.startswith("Human:"):
                formatted_prompt = f"Human: {prompt}\n\nAssistant:"
            else:
                formatted_prompt = prompt

            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)

            # ä¼˜åŒ–ç”Ÿæˆå‚æ•° - ä¸“ä¸ºæ•™å­¦é—®ç­”ä¼˜åŒ–
            gen_kwargs = {
                "max_new_tokens": min(150, max_new_tokens),  # é™åˆ¶é•¿åº¦ï¼Œé¿å…å†—é•¿å›å¤
                "temperature": 0.3,  # é™ä½éšæœºæ€§ï¼Œæé«˜ä¸€è‡´æ€§
                "top_p": 0.8,  # æ›´ä¿å®ˆçš„nucleus sampling
                "top_k": 30,  # å‡å°‘å€™é€‰è¯æ•°é‡
                "do_sample": True,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "repetition_penalty": 1.3,  # å¢å¼ºé‡å¤æƒ©ç½š
                "length_penalty": 1.0,  # é•¿åº¦æƒ©ç½š
                "early_stopping": True,  # å¯ç”¨æ—©åœ
                "no_repeat_ngram_size": 3,  # é¿å…3-gramé‡å¤
            }

            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                output_ids = self.peft_model.generate(**inputs, **gen_kwargs)

            # è§£ç è¾“å‡º - åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
            new_tokens = output_ids[0][inputs['input_ids'].shape[1]:]
            response = self.tokenizer.decode(new_tokens, skip_special_tokens=True)

            # åå¤„ç†ï¼šæ¸…ç†å’Œä¼˜åŒ–å›å¤
            response = self._post_process_response(response)

            return response

        except Exception as e:
            logger.error(f"ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"

    def _post_process_response(self, response):
        """åå¤„ç†ç”Ÿæˆçš„å›å¤"""
        if not response:
            return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰ç†è§£æ‚¨çš„é—®é¢˜ã€‚"

        # ç§»é™¤å¤šä½™çš„ç©ºç™½
        response = response.strip()

        # å¦‚æœå“åº”ä¸ºç©ºæˆ–è¿‡çŸ­
        if len(response) < 3:
            return "æŠ±æ­‰ï¼Œæˆ‘éœ€è¦æ›´å¤šä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"

        # ç§»é™¤å¯èƒ½çš„é‡å¤å¥å­
        sentences = response.split('ã€‚')
        unique_sentences = []
        seen = set()

        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and sentence not in seen and len(sentence) > 5:
                unique_sentences.append(sentence)
                seen.add(sentence)

        # é‡æ–°ç»„åˆå¥å­
        if unique_sentences:
            cleaned_response = 'ã€‚'.join(unique_sentences)
            if not cleaned_response.endswith('ã€‚') and not cleaned_response.endswith(
                    'ï¼') and not cleaned_response.endswith('ï¼Ÿ'):
                cleaned_response += 'ã€‚'
            return cleaned_response

        return response

# å½“è„šæœ¬ç›´æ¥è¿è¡Œæ—¶ï¼Œæ‰§è¡Œç¤ºä¾‹è®­ç»ƒè¿‡ç¨‹
if __name__ == "__main__":
    # åˆ›å»ºLoRAå¾®è°ƒå™¨å®ä¾‹
    fine_tuner = LoRAFineTuner()

    # åŠ è½½åŸºç¡€æ¨¡å‹
    if fine_tuner.load_base_model():
        # å‡†å¤‡LoRAé…ç½®
        fine_tuner.prepare_lora_config()

        # å‡†å¤‡è®­ç»ƒæ•°æ®é›†ï¼ˆä½¿ç”¨é»˜è®¤ç¤ºä¾‹æ•°æ®ï¼‰
        dataset = fine_tuner.prepare_dataset()

        # è®­ç»ƒæ¨¡å‹
        fine_tuner.train(dataset, epochs=3)

        # æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹
        test_prompts = [
            "Pythonä¸­forå¾ªç¯å’Œwhileå¾ªç¯æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
            "äººå·¥æ™ºèƒ½åœ¨æ•™è‚²ä¸­æœ‰å“ªäº›åº”ç”¨ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯å¤šæ¨¡æ€äº¤äº’ï¼Ÿ",
            "PEPPERæœºå™¨äººåœ¨è¯¾å ‚ä¸Šçš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]

        for prompt in test_prompts:
            response = fine_tuner.generate_response(prompt)
            print(f"é—®é¢˜: {prompt}")
            print(f"å›ç­”: {response}")
            print("-" * 80)