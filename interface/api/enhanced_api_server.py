#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ•™å­¦ç³»ç»Ÿ - å¢å¼ºAPIæœåŠ¡å™¨
æ”¯æŒå¤§è¯­è¨€æ¨¡å‹ã€çŸ¥è¯†å›¾è°±ã€å¤šæ¨¡æ€äº¤äº’ã€æ™ºèƒ½æ•™å­¦å››å¤§åŠŸèƒ½æ¨¡å—
"""
import gc
import json
import logging
import os
import sys
from datetime import datetime

import torch
from flask import Flask, request, jsonify
from flask_cors import CORS
from werkzeug.utils import secure_filename

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

# å¯¼å…¥å„ä¸ªæ¨¡å—
from ai_service.llm_module.llm_interface import LLMService
from ai_service.llm_module.lora_fine_tuning import LoRAFineTuner
from ai_service.knowledge_graph.knowledge_graph import KnowledgeGraph
from ai_service.knowledge_graph.education_knowledge_processor import EducationKnowledgeProcessor
from ai_service.multimodal.speech_recognition import SpeechRecognizer
from ai_service.multimodal.image_recognition import ImageRecognizer
from ai_service.multimodal.text_processor import TextProcessor

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("PEPPER_API")

app = Flask(__name__)
CORS(app)

# å…¨å±€å˜é‡å­˜å‚¨æœåŠ¡å®ä¾‹
services = {
    'llm': None,
    'fine_tuner': None,
    'knowledge_graph': None,
    'knowledge_processor': None,
    'speech_recognizer': None,
    'image_recognizer': None,
    'text_processor': None,
    'teaching': None
}

# ç³»ç»ŸçŠ¶æ€
system_status = {
    'model_loaded': False,
    'model_path': None,
    'model_quantization': None,
    'neo4j_connected': False,
    'training_progress': 0,
    'training_active': False,
    'training_start_time': None,
    'training_end_time': None,
    'training_output_dir': None,
    'training_config': None,
    'training_error': None,
    'auto_cleanup_enabled': True  # æ–°å¢ï¼šè‡ªåŠ¨æ¸…ç†å¼€å…³
}

# é…ç½®
UPLOAD_FOLDER = 'uploads'
REPORTS_FOLDER = 'reports'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(REPORTS_FOLDER, exist_ok=True)

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size


def force_cleanup_model(model_service=None):
    """å¼ºåˆ¶æ¸…ç†æ¨¡å‹å’Œé‡Šæ”¾å†…å­˜"""
    try:
        if model_service:
            # æ¸…ç†æŒ‡å®šæ¨¡å‹
            if hasattr(model_service, 'model') and model_service.model:
                del model_service.model
            if hasattr(model_service, 'peft_model') and model_service.peft_model:
                del model_service.peft_model
            if hasattr(model_service, 'tokenizer') and model_service.tokenizer:
                del model_service.tokenizer

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()

        # æ¸…ç†CUDAç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

        logger.info("æ¨¡å‹å†…å­˜æ¸…ç†å®Œæˆ")
        return True

    except Exception as e:
        logger.error(f"å¼ºåˆ¶æ¸…ç†æ¨¡å‹å¤±è´¥: {e}")
        return False

def initialize_services():
    """åˆå§‹åŒ–æœåŠ¡"""
    try:
        # åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨
        services['text_processor'] = TextProcessor()

        # åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å™¨
        services['speech_recognizer'] = SpeechRecognizer()

        # åˆå§‹åŒ–å›¾åƒè¯†åˆ«å™¨
        services['image_recognizer'] = ImageRecognizer()

        logger.info("åŸºç¡€æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        logger.error(f"æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")

# ======================== å¤§è¯­è¨€æ¨¡å‹API ========================

@app.route('/api/load_model', methods=['POST'])
def load_model():
    """åŠ è½½å¤§è¯­è¨€æ¨¡å‹ - ç®€åŒ–ä¿®å¤ç‰ˆæœ¬"""
    try:
        data = request.json
        if not data:
            return jsonify({"status": "error", "message": "è¯·æ±‚æ•°æ®ä¸ºç©º"})

        model_path = data.get('model_path', '')
        use_4bit = data.get('use_4bit', True)
        use_8bit = data.get('use_8bit', False)

        if not model_path or model_path.strip() == '':
            return jsonify({"status": "error", "message": "æ¨¡å‹è·¯å¾„ä¸èƒ½ä¸ºç©º"})

        logger.info(f"æ­£åœ¨åŠ è½½æ¨ç†æ¨¡å‹: {model_path}")

        if not os.path.exists(model_path):
            return jsonify({"status": "error", "message": f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}"})

        # æ¸…ç†ç°æœ‰æ¨¡å‹
        if services['llm']:
            logger.info("æ¸…ç†ç°æœ‰æ¨ç†æ¨¡å‹")
            force_cleanup_model(services['llm'])
            services['llm'] = None

        if services['fine_tuner']:
            logger.info("æ¸…ç†è®­ç»ƒæ¨¡å‹")
            force_cleanup_model(services['fine_tuner'])
            services['fine_tuner'] = None

        # æ£€æµ‹æ˜¯å¦æ˜¯LoRAæ¨¡å‹
        adapter_files = ['adapter_config.json', 'adapter_model.bin', 'adapter_model.safetensors']
        is_lora_model = any(os.path.exists(os.path.join(model_path, f)) for f in adapter_files)

        try:
            if is_lora_model:
                logger.info("æ£€æµ‹åˆ°LoRAæ¨¡å‹ï¼Œä½¿ç”¨LoRAåŠ è½½æ–¹å¼")

                # ç¡®å®šåŸºç¡€æ¨¡å‹è·¯å¾„
                base_model_path = "models/deepseek-coder-1.3b-base"

                # å°è¯•ä»training_info.jsonè¯»å–åŸºç¡€æ¨¡å‹è·¯å¾„
                training_info_path = os.path.join(model_path, 'training_info.json')
                if os.path.exists(training_info_path):
                    try:
                        with open(training_info_path, 'r', encoding='utf-8') as f:
                            training_info = json.load(f)
                            saved_base_path = training_info.get('base_model')
                            if saved_base_path and os.path.exists(saved_base_path):
                                base_model_path = saved_base_path
                                logger.info(f"ä½¿ç”¨ä¿å­˜çš„åŸºç¡€æ¨¡å‹è·¯å¾„: {base_model_path}")
                    except:
                        pass

                if not os.path.exists(base_model_path):
                    return jsonify({
                        "status": "error",
                        "message": f"åŸºç¡€æ¨¡å‹ä¸å­˜åœ¨: {base_model_path}"
                    })

                # åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨
                from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

                logger.info("åŠ è½½åˆ†è¯å™¨...")
                tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token

                # é…ç½®é‡åŒ–
                quantization_config = None
                device = "cuda" if torch.cuda.is_available() else "cpu"
                dtype = torch.float16 if torch.cuda.is_available() else torch.float32

                if device == "cuda" and (use_4bit or use_8bit):
                    if use_4bit:
                        logger.info("ä½¿ç”¨4bité‡åŒ–")
                        quantization_config = BitsAndBytesConfig(
                            load_in_4bit=True,
                            bnb_4bit_quant_type="nf4",
                            bnb_4bit_compute_dtype=torch.float16,
                            bnb_4bit_use_double_quant=True,
                        )
                    elif use_8bit:
                        logger.info("ä½¿ç”¨8bité‡åŒ–")
                        quantization_config = BitsAndBytesConfig(load_in_8bit=True)

                # åŠ è½½åŸºç¡€æ¨¡å‹
                logger.info(f"åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
                load_kwargs = {"trust_remote_code": True, "torch_dtype": dtype}

                if quantization_config:
                    load_kwargs["quantization_config"] = quantization_config
                    load_kwargs["device_map"] = "auto"
                elif device == "cuda":
                    load_kwargs["device_map"] = "auto"

                base_model = AutoModelForCausalLM.from_pretrained(base_model_path, **load_kwargs)

                # åŠ è½½LoRAé€‚é…å™¨
                logger.info(f"åŠ è½½LoRAé€‚é…å™¨: {model_path}")
                from peft import PeftModel
                peft_model = PeftModel.from_pretrained(base_model, model_path)

                # åˆ›å»ºç®€å•çš„åŒ…è£…ç±»
                class SimpleLoRAWrapper:
                    def __init__(self, peft_model, tokenizer):
                        self.peft_model = peft_model
                        self.model = peft_model
                        self.tokenizer = tokenizer

                    def generate_response(self, prompt, max_length=512):
                        try:
                            # ç®€åŒ–è¾“å…¥æ ¼å¼
                            if not prompt.startswith("Human:"):
                                formatted_prompt = f"Human: {prompt}\n\nAssistant:"
                            else:
                                formatted_prompt = prompt

                            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.peft_model.device)

                            with torch.no_grad():
                                outputs = self.peft_model.generate(
                                    **inputs,
                                    max_new_tokens=80,  # è¿›ä¸€æ­¥é™åˆ¶é•¿åº¦
                                    temperature=0.2,  # æ›´ä½çš„æ¸©åº¦
                                    top_p=0.9,
                                    top_k=20,
                                    do_sample=True,
                                    pad_token_id=self.tokenizer.eos_token_id,
                                    eos_token_id=self.tokenizer.eos_token_id,
                                    repetition_penalty=1.5,  # æ›´å¼ºçš„é‡å¤æƒ©ç½š
                                    early_stopping=True,
                                    no_repeat_ngram_size=2,
                                )

                            # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
                            new_tokens = outputs[0][inputs['input_ids'].shape[1]:]
                            response = self.tokenizer.decode(new_tokens, skip_special_tokens=True)

                            # ç®€å•æ¸…ç†
                            response = response.strip()

                            # ç§»é™¤æ˜æ˜¾çš„é‡å¤æˆ–æ— æ„ä¹‰å†…å®¹
                            if len(response) < 3:
                                return "è¯·å…·ä½“è¯´æ˜æ‚¨çš„é—®é¢˜ã€‚"

                            # ç¡®ä¿å›å¤å®Œæ•´
                            if not response.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
                                if len(response) > 10:
                                    response += 'ã€‚'

                            return response[:200]  # ç¡¬æ€§é™åˆ¶æœ€å¤§é•¿åº¦

                        except Exception as e:
                            logger.error(f"ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
                            return "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"

                services['llm'] = SimpleLoRAWrapper(peft_model, tokenizer)
                logger.info("LoRAæ¨¡å‹åŠ è½½å®Œæˆ")

            else:
                # åŸå§‹æ¨¡å‹
                logger.info("æ£€æµ‹åˆ°åŸå§‹æ¨¡å‹ï¼Œä½¿ç”¨LLMService")
                services['llm'] = LLMService(model_path)

            # æ›´æ–°çŠ¶æ€
            system_status['model_loaded'] = True
            system_status['model_path'] = model_path
            system_status['model_quantization'] = '4bit' if use_4bit else '8bit' if use_8bit else 'full'

            # æµ‹è¯•æ¨¡å‹
            try:
                test_response = services['llm'].generate_response("ä½ å¥½", 20)
                logger.info(f"æ¨¡å‹æµ‹è¯•æˆåŠŸ: {test_response[:30]}...")
            except Exception as test_error:
                logger.warning(f"æ¨¡å‹æµ‹è¯•å¤±è´¥: {test_error}")

            return jsonify({
                "status": "success",
                "message": "æ¨¡å‹åŠ è½½æˆåŠŸ",
                "model_path": model_path,
                "quantization": system_status['model_quantization'],
                "model_type": "LoRA" if is_lora_model else "Base"
            })

        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return jsonify({"status": "error", "message": f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"})

    except Exception as e:
        logger.error(f"load_modelå‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        return jsonify({"status": "error", "message": f"åŠ è½½å¤±è´¥: {str(e)}"})

@app.route('/api/unload_model', methods=['POST'])
def unload_model():
    """å¸è½½å¤§è¯­è¨€æ¨¡å‹"""
    try:
        logger.info("å¼€å§‹å¸è½½æ¨¡å‹...")

        # æ¸…ç†æ¨ç†æ¨¡å‹
        if services['llm']:
            force_cleanup_model(services['llm'])
            services['llm'] = None

        # æ¸…ç†è®­ç»ƒæ¨¡å‹
        if services['fine_tuner']:
            force_cleanup_model(services['fine_tuner'])
            services['fine_tuner'] = None

        system_status['model_loaded'] = False
        system_status['model_path'] = None
        system_status['model_quantization'] = None

        logger.info("æ‰€æœ‰æ¨¡å‹å·²å¸è½½")
        return jsonify({
            "status": "success",
            "message": "æ¨¡å‹å·²å¸è½½"
        })

    except Exception as e:
        logger.error(f"æ¨¡å‹å¸è½½å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })

@app.route('/api/start_finetuning', methods=['POST'])
def start_finetuning():
    """å¼€å§‹LoRAå¾®è°ƒ"""
    try:
        if 'training_data' not in request.files:
            return jsonify({
                "status": "error",
                "message": "æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶"
            })

        file = request.files['training_data']
        model_path = request.form.get('model_path', '')
        epochs = int(request.form.get('epochs', 3))
        learning_rate = float(request.form.get('learning_rate', 0.0002))
        batch_size = int(request.form.get('batch_size', 2))  # 4bitè®­ç»ƒé»˜è®¤batch_size=2
        use_4bit = request.form.get('use_4bit', 'true').lower() == 'true'
        use_8bit = request.form.get('use_8bit', 'false').lower() == 'true'

        if file.filename == '':
            return jsonify({
                "status": "error",
                "message": "æœªé€‰æ‹©æ–‡ä»¶"
            })

        if not model_path or model_path.strip() == '':
            return jsonify({
                "status": "error",
                "message": "æœªæŒ‡å®šæ¨¡å‹è·¯å¾„"
            })

        # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            return jsonify({
                "status": "error",
                "message": f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}"
            })

        # æ¸…ç†ç°æœ‰çš„æ¨ç†æ¨¡å‹ï¼Œä¸ºè®­ç»ƒè®©å‡ºå†…å­˜
        if services['llm']:
            logger.info("æ¸…ç†æ¨ç†æ¨¡å‹ï¼Œä¸ºè®­ç»ƒè®©å‡ºå†…å­˜")
            force_cleanup_model(services['llm'])
            services['llm'] = None
            system_status['model_loaded'] = False

        # ä¿å­˜æ–‡ä»¶
        filename = secure_filename(file.filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(filepath)

        # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        quantization_suffix = "_4bit" if use_4bit else "_8bit" if use_8bit else ""
        output_dir = f"models/deepseek-{timestamp}{quantization_suffix}"

        # åˆ›å»ºå¾®è°ƒå™¨ï¼Œä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹è·¯å¾„
        services['fine_tuner'] = LoRAFineTuner(
            base_model_path=model_path,  # ä½¿ç”¨å‰ç«¯ä¼ é€’çš„æ¨¡å‹è·¯å¾„
            output_dir=output_dir
        )

        # è®°å½•è®­ç»ƒä¿¡æ¯åˆ°ç³»ç»ŸçŠ¶æ€
        system_status['training_output_dir'] = output_dir
        system_status['training_start_time'] = datetime.now().isoformat()
        system_status['training_active'] = True
        system_status['training_progress'] = 0
        system_status['training_config'] = {
            'model_path': model_path,
            'epochs': epochs,
            'batch_size': batch_size,
            'learning_rate': learning_rate,
            'use_4bit': use_4bit,
            'use_8bit': use_8bit,
            'quantization': '4bit' if use_4bit else '8bit' if use_8bit else 'none'
        }

        # å¯åŠ¨å¼‚æ­¥è®­ç»ƒä»»åŠ¡
        import threading
        def run_training():
            try:
                logger.info(f"å¼€å§‹{system_status['training_config']['quantization']}é‡åŒ–è®­ç»ƒ")

                # åŠ è½½åŸºç¡€æ¨¡å‹(0-10%)
                system_status['training_progress'] = 5
                success = services['fine_tuner'].load_base_model(use_4bit=use_4bit, use_8bit=use_8bit)
                if not success:
                    system_status['training_active'] = False
                    system_status['training_error'] = "æ¨¡å‹åŠ è½½å¤±è´¥"
                    logger.error("è®­ç»ƒæ¨¡å‹åŠ è½½å¤±è´¥")
                    return

                system_status['training_progress'] = 10
                logger.info("è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‡†å¤‡LoRAé…ç½®")

                # å‡†å¤‡LoRAé…ç½® (10-15%)
                services['fine_tuner'].prepare_lora_config()
                system_status['training_progress'] = 12

                logger.info("å‡†å¤‡æ•°æ®é›†")
                # å‡†å¤‡æ•°æ®é›†
                dataset = services['fine_tuner'].prepare_dataset(data_path=filepath)
                if dataset is None:
                    system_status['training_active'] = False
                    system_status['training_error'] = "æ•°æ®é›†å‡†å¤‡å¤±è´¥"
                    logger.error("æ•°æ®é›†å‡†å¤‡å¤±è´¥")
                    return

                system_status['training_progress'] = 15
                logger.info("å¼€å§‹LoRAå¾®è°ƒè®­ç»ƒ")

                # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•° - è¿™ä¸ªå‡½æ•°ä¼šè¢«è®­ç»ƒå™¨è°ƒç”¨
                def training_progress_callback(progress):
                    # progress æ˜¯ 0-100 çš„çœŸå®è®­ç»ƒè¿›åº¦
                    # æ˜ å°„åˆ° 15-99 çš„åŒºé—´
                    mapped_progress = 15 + (progress * 0.84)  # 84% = 99-15
                    system_status['training_progress'] = int(min(99, mapped_progress))
                    logger.info(f"è®­ç»ƒè¿›åº¦: {system_status['training_progress']}%")

                # å¼€å§‹è®­ç»ƒ
                success = services['fine_tuner'].train(
                    dataset,
                    epochs=epochs,
                    batch_size=batch_size,
                    learning_rate=learning_rate,
                    progress_callback=training_progress_callback
                )

                if success:
                    system_status['training_progress'] = 100
                    system_status['training_end_time'] = datetime.now().isoformat()
                    logger.info(f"è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")

                    # è®­ç»ƒå®Œæˆåçš„è‡ªåŠ¨æ¸…ç†
                    logger.info("è®­ç»ƒå®Œæˆï¼Œå¼€å§‹è‡ªåŠ¨æ¸…ç†è®­ç»ƒæ¨¡å‹...")

                    # ç­‰å¾…ç¡®ä¿æ¨¡å‹ä¿å­˜å®Œæˆ
                    import time
                    time.sleep(3)  # å¢åŠ ç­‰å¾…æ—¶é—´åˆ°3ç§’

                    # æ‰§è¡Œæ¸…ç†
                    try:
                        if services['fine_tuner']:
                            force_cleanup_model(services['fine_tuner'])
                            services['fine_tuner'] = None
                            logger.info("âœ… è®­ç»ƒæ¨¡å‹å·²è‡ªåŠ¨æ¸…ç†ï¼Œå†…å­˜å·²é‡Šæ”¾")
                        else:
                            logger.info("è®­ç»ƒæ¨¡å‹å·²ç»ä¸ºç©ºï¼Œæ— éœ€æ¸…ç†")
                    except Exception as cleanup_error:
                        logger.error(f"è‡ªåŠ¨æ¸…ç†å¤±è´¥: {cleanup_error}")

                else:
                    system_status['training_error'] = "è®­ç»ƒè¿‡ç¨‹å¤±è´¥"
                    logger.error("è®­ç»ƒè¿‡ç¨‹å¤±è´¥")

            except Exception as e:
                logger.error(f"è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
                system_status['training_error'] = str(e)
            finally:
                system_status['training_active'] = False
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.remove(filepath)
                    logger.info("è®­ç»ƒæ•°æ®ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
                except:
                    pass

                # æœ€ç»ˆæ¸…ç†æ£€æŸ¥
                try:
                    if services['fine_tuner'] is not None:
                        logger.info("æ‰§è¡Œæœ€ç»ˆä¿é™©æ¸…ç†...")
                        force_cleanup_model(services['fine_tuner'])
                        services['fine_tuner'] = None
                        logger.info("æœ€ç»ˆæ¸…ç†å®Œæˆ")
                except Exception as final_cleanup_error:
                    logger.error(f"æœ€ç»ˆæ¸…ç†å¤±è´¥: {final_cleanup_error}")

        # å¯åŠ¨è®­ç»ƒçº¿ç¨‹
        thread = threading.Thread(target=run_training)
        thread.daemon = True
        thread.start()


        logger.info(f"LoRAå¾®è°ƒå·²å¼€å§‹ï¼Œè¾“å‡ºç›®å½•: {output_dir}")
        return jsonify({
            "status": "success",
            "message": "å¾®è°ƒå·²å¼€å§‹",
            "output_dir": output_dir,
            "timestamp": timestamp,
            "config": system_status['training_config']
        })

    except Exception as e:
        logger.error(f"å¾®è°ƒå¯åŠ¨å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })

@app.route('/api/training_progress', methods=['GET'])
def get_training_progress():
    """è·å–è®­ç»ƒè¿›åº¦"""
    progress_info = {
        "status": "success",
        "progress": system_status['training_progress'],
        "active": system_status['training_active'],
        "output_dir": system_status.get('training_output_dir', ''),
        "start_time": system_status.get('training_start_time', ''),
        "config": system_status.get('training_config', {}),
        "auto_cleanup_enabled": system_status.get('auto_cleanup_enabled', True)
    }

    # æ·»åŠ ç»“æŸæ—¶é—´ï¼ˆå¦‚æœæœ‰ï¼‰
    if 'training_end_time' in system_status:
        progress_info["end_time"] = system_status['training_end_time']

    # æ·»åŠ é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    if 'training_error' in system_status:
        progress_info["error"] = system_status['training_error']

    # è®¡ç®—é¢„ä¼°å‰©ä½™æ—¶é—´
    if system_status['training_active'] and 'training_start_time' in system_status:
        try:
            start_time = datetime.fromisoformat(system_status['training_start_time'])
            elapsed_time = (datetime.now() - start_time).total_seconds()
            progress = system_status['training_progress']

            if progress > 5:  # é¿å…é™¤é›¶é”™è¯¯
                estimated_total_time = elapsed_time * 100 / progress
                remaining_time = estimated_total_time - elapsed_time
                progress_info["estimated_remaining_seconds"] = max(0, int(remaining_time))
        except:
            pass

    # æ·»åŠ å†…å­˜ä½¿ç”¨ä¿¡æ¯
    try:
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3
            gpu_allocated = torch.cuda.memory_allocated() / 1024 ** 3
            gpu_cached = torch.cuda.memory_reserved() / 1024 ** 3

            progress_info["memory_info"] = {
                "gpu_total_gb": round(gpu_memory, 2),
                "gpu_allocated_gb": round(gpu_allocated, 2),
                "gpu_cached_gb": round(gpu_cached, 2),
                "gpu_free_gb": round(gpu_memory - gpu_cached, 2)
            }
    except:
        pass

    return jsonify(progress_info)

@app.route('/api/toggle_auto_cleanup', methods=['POST'])
def toggle_auto_cleanup():
    """åˆ‡æ¢è®­ç»ƒå®Œæˆåçš„è‡ªåŠ¨æ¸…ç†åŠŸèƒ½"""
    try:
        data = request.json
        enabled = data.get('enabled', True)

        system_status['auto_cleanup_enabled'] = enabled

        logger.info(f"è‡ªåŠ¨æ¸…ç†åŠŸèƒ½å·²{'å¯ç”¨' if enabled else 'ç¦ç”¨'}")

        return jsonify({
            "status": "success",
            "message": f"è‡ªåŠ¨æ¸…ç†åŠŸèƒ½å·²{'å¯ç”¨' if enabled else 'ç¦ç”¨'}",
            "auto_cleanup_enabled": enabled
        })

    except Exception as e:
        logger.error(f"åˆ‡æ¢è‡ªåŠ¨æ¸…ç†åŠŸèƒ½å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })

@app.route('/api/manual_cleanup', methods=['POST'])
def manual_cleanup():
    """æ‰‹åŠ¨æ¸…ç†æ¨¡å‹å†…å­˜"""
    try:
        cleanup_results = []

        # æ¸…ç†æ¨ç†æ¨¡å‹
        if services['llm']:
            force_cleanup_model(services['llm'])
            services['llm'] = None
            system_status['model_loaded'] = False
            cleanup_results.append("æ¨ç†æ¨¡å‹å·²æ¸…ç†")

        # æ¸…ç†è®­ç»ƒæ¨¡å‹
        if services['fine_tuner']:
            force_cleanup_model(services['fine_tuner'])
            services['fine_tuner'] = None
            cleanup_results.append("è®­ç»ƒæ¨¡å‹å·²æ¸…ç†")

        if not cleanup_results:
            cleanup_results.append("æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ¨¡å‹")

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            cleanup_results.append("GPUç¼“å­˜å·²æ¸…ç†")

        logger.info(f"æ‰‹åŠ¨æ¸…ç†å®Œæˆ: {', '.join(cleanup_results)}")

        return jsonify({
            "status": "success",
            "message": "æ‰‹åŠ¨æ¸…ç†å®Œæˆ",
            "details": cleanup_results
        })

    except Exception as e:
        logger.error(f"æ‰‹åŠ¨æ¸…ç†å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })

@app.route('/api/system_memory_info', methods=['GET'])
def get_system_memory_info():
    """è·å–ç³»ç»Ÿå†…å­˜ä½¿ç”¨ä¿¡æ¯"""
    try:
        import psutil

        memory_info = {
            "cpu_memory": {
                "total_gb": round(psutil.virtual_memory().total / 1024 ** 3, 2),
                "available_gb": round(psutil.virtual_memory().available / 1024 ** 3, 2),
                "used_gb": round(psutil.virtual_memory().used / 1024 ** 3, 2),
                "percent": psutil.virtual_memory().percent
            }
        }

        # GPUå†…å­˜ä¿¡æ¯
        if torch.cuda.is_available():
            memory_info["gpu_memory"] = {
                "total_gb": round(torch.cuda.get_device_properties(0).total_memory / 1024 ** 3, 2),
                "allocated_gb": round(torch.cuda.memory_allocated() / 1024 ** 3, 2),
                "cached_gb": round(torch.cuda.memory_reserved() / 1024 ** 3, 2),
                "free_gb": round(
                    (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1024 ** 3, 2)
            }
        else:
            memory_info["gpu_memory"] = {"available": False}

        # æ¨¡å‹çŠ¶æ€
        memory_info["model_status"] = {
            "inference_model_loaded": services['llm'] is not None,
            "training_model_loaded": services['fine_tuner'] is not None,
            "training_active": system_status['training_active']
        }

        return jsonify({
            "status": "success",
            "memory_info": memory_info
        })

    except Exception as e:
        logger.error(f"è·å–å†…å­˜ä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })

# ======================== å…¶ä»–APIä¿æŒä¸å˜ ========================

@app.route('/api/test_model', methods=['POST'])
def test_model():
    """æµ‹è¯•å¤§è¯­è¨€æ¨¡å‹"""
    try:
        if not services['llm']:
            return jsonify({
                "status": "error",
                "message": "æ¨¡å‹æœªåŠ è½½"
            })

        data = request.json
        question = data.get('question', '')
        max_tokens = data.get('max_tokens', 512)
        temperature = data.get('temperature', 0.7)

        if not question.strip():
            return jsonify({
                "status": "error",
                "message": "é—®é¢˜ä¸èƒ½ä¸ºç©º"
            })

        # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”
        try:
            if hasattr(services['llm'], 'generate_response'):
                response = services['llm'].generate_response(question, max_tokens)
            else:
                logger.error(f"æ¨¡å‹å¯¹è±¡æ²¡æœ‰ generate_response æ–¹æ³•: {type(services['llm'])}")
                return jsonify({
                    "status": "error",
                    "message": "æ¨¡å‹æ¥å£ä¸å…¼å®¹"
                })

            return jsonify({
                "status": "success",
                "result": response,  # æ³¨æ„ï¼šè¿™é‡Œç”¨ resultï¼Œå‰ç«¯æœŸæœ›è¿™ä¸ªå­—æ®µ
                "model_quantization": system_status.get('model_quantization', 'unknown'),
                "model_type": type(services['llm']).__name__
            })

        except Exception as model_error:
            logger.error(f"æ¨¡å‹æ¨ç†å¤±è´¥: {model_error}")
            return jsonify({
                "status": "error",
                "message": f"æ¨¡å‹æ¨ç†å¤±è´¥: {str(model_error)}"
            })

    except Exception as e:
        logger.error(f"æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })

@app.route('/api/debug_model_status', methods=['GET'])
def debug_model_status():
    """è°ƒè¯•æ¨¡å‹çŠ¶æ€"""
    try:
        debug_info = {
            "services_llm_exists": services['llm'] is not None,
            "services_llm_type": type(services['llm']).__name__ if services['llm'] else None,
            "system_status_model_loaded": system_status['model_loaded'],
            "system_status_model_path": system_status.get('model_path'),
            "system_status_quantization": system_status.get('model_quantization'),
        }

        # æ£€æŸ¥æ¨¡å‹æ–¹æ³•
        if services['llm']:
            debug_info["model_methods"] = [method for method in dir(services['llm']) if not method.startswith('_')]
            debug_info["has_generate_response"] = hasattr(services['llm'], 'generate_response')

        # æ£€æŸ¥çŸ¥è¯†å›¾è°±çŠ¶æ€
        debug_info["neo4j_connected"] = system_status['neo4j_connected']
        debug_info["knowledge_graph_exists"] = services['knowledge_graph'] is not None

        return jsonify({
            "status": "success",
            "debug_info": debug_info
        })

    except Exception as e:
        logger.error(f"è°ƒè¯•çŠ¶æ€è·å–å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })

# ======================== çŸ¥è¯†å›¾è°±API ========================

@app.route('/api/connect_neo4j', methods=['POST'])
def connect_neo4j():
    """è¿æ¥Neo4jæ•°æ®åº“"""
    try:
        data = request.json
        uri = data.get('uri', 'bolt://localhost:7687')
        user = data.get('user', 'neo4j')
        password = data.get('password', 'password')

        services['knowledge_graph'] = KnowledgeGraph(uri, user, password)
        services['knowledge_processor'] = EducationKnowledgeProcessor(uri, user, password)

        # æµ‹è¯•è¿æ¥
        test_result = services['knowledge_graph'].query("RETURN 1 as test")
        if test_result:
            system_status['neo4j_connected'] = True
            logger.info("Neo4jè¿æ¥æˆåŠŸ")
            return jsonify({
                "status": "success",
                "message": "æ•°æ®åº“è¿æ¥æˆåŠŸ"
            })
        else:
            raise Exception("è¿æ¥æµ‹è¯•å¤±è´¥")

    except Exception as e:
        logger.error(f"Neo4jè¿æ¥å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })

@app.route('/api/generate_sample_knowledge', methods=['POST'])
def generate_sample_knowledge():
    """ç”Ÿæˆç¤ºä¾‹çŸ¥è¯†"""
    try:
        if not services['knowledge_processor']:
            return jsonify({
                "status": "error",
                "message": "æœªè¿æ¥çŸ¥è¯†å›¾è°±æ•°æ®åº“"
            })

        count = services['knowledge_processor'].create_educational_knowledge_base()

        logger.info("ç¤ºä¾‹çŸ¥è¯†ç”ŸæˆæˆåŠŸ")
        return jsonify({
            "status": "success",
            "message": "ç¤ºä¾‹çŸ¥è¯†ç”ŸæˆæˆåŠŸ",
            "count": count
        })

    except Exception as e:
        logger.error(f"ç¤ºä¾‹çŸ¥è¯†ç”Ÿæˆå¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })

@app.route('/api/chat', methods=['POST'])
def chat():
    """æ™ºèƒ½å¯¹è¯"""
    try:
        data = request.json
        message = data.get('message', '')
        use_knowledge_graph = data.get('use_knowledge_graph', False)

        if not message.strip():
            return jsonify({
                "status": "error",
                "message": "æ¶ˆæ¯ä¸èƒ½ä¸ºç©º"
            })

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŠ è½½
        if not services['llm']:
            return jsonify({
                "status": "error",
                "message": "æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆåœ¨å·¦ä¾§é¢æ¿åŠ è½½æ¨¡å‹"
            })

            # æ£€æŸ¥æ¨¡å‹ç±»å‹å¹¶è°ƒç”¨ç›¸åº”çš„ç”Ÿæˆæ–¹æ³•
        try:
            if hasattr(services['llm'], 'generate_response'):
                # æ ‡å‡† LLMService æˆ– LoRAFineTuner éƒ½æœ‰è¿™ä¸ªæ–¹æ³•
                if use_knowledge_graph and services['knowledge_graph'] and system_status['neo4j_connected']:
                    # ä½¿ç”¨çŸ¥è¯†å›¾è°±å¢å¼ºå›ç­”
                    response = chat_with_knowledge_graph(message)
                else:
                    # ç›´æ¥ä½¿ç”¨æ¨¡å‹å›ç­”
                    response = services['llm'].generate_response(message)
            else:
                logger.error(f"æ¨¡å‹å¯¹è±¡ç±»å‹é”™è¯¯: {type(services['llm'])}")
                return jsonify({
                    "status": "error",
                    "message": f"æ¨¡å‹å¯¹è±¡ç±»å‹ä¸æ”¯æŒ: {type(services['llm'])}"
                })

            return jsonify({
                "status": "success",
                "response": response,
                "model_type": type(services['llm']).__name__,
                "use_knowledge_graph": use_knowledge_graph and system_status['neo4j_connected']
            })

        except Exception as model_error:
            logger.error(f"æ¨¡å‹æ¨ç†å¤±è´¥: {model_error}")
            return jsonify({
                "status": "error",
                "message": f"æ¨¡å‹æ¨ç†å¤±è´¥: {str(model_error)}"
            })

    except Exception as e:
        logger.error(f"æ™ºèƒ½å¯¹è¯å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })

def chat_with_knowledge_graph(message):
    """ç»“åˆçŸ¥è¯†å›¾è°±ç”Ÿæˆå›ç­”"""
    try:
        # æå–å…³é”®è¯ (ç®€å•ç‰ˆæœ¬)
        keywords = []
        if services['text_processor']:
            keywords = services['text_processor'].extract_keywords(message)
        else:
            # ç®€å•çš„å…³é”®è¯æå–
            import jieba
            words = jieba.cut(message)
            keywords = [word for word in words if len(word) > 1][:5]

        # ä»çŸ¥è¯†å›¾è°±æŸ¥è¯¢ç›¸å…³ä¿¡æ¯
        knowledge_items = []
        for keyword in keywords:
            try:
                items = services['knowledge_graph'].find_related_knowledge(keyword)
                knowledge_items.extend(items)
            except Exception as e:
                logger.warning(f"æŸ¥è¯¢çŸ¥è¯†å›¾è°±å¤±è´¥ (å…³é”®è¯: {keyword}): {e}")

        # æ„å»ºçŸ¥è¯†ä¸Šä¸‹æ–‡
        knowledge_context = ""
        if knowledge_items:
            knowledge_context = "\nç›¸å…³çŸ¥è¯†:\n"
            for item in knowledge_items[:5]:  # é™åˆ¶çŸ¥è¯†æ¡ç›®æ•°é‡
                if isinstance(item, dict) and 'start_node' in item and 'end_node' in item:
                    start_name = item['start_node'].get('name', '')
                    relationship = item.get('relationship', '')
                    end_name = item['end_node'].get('name', '')
                    knowledge_context += f"- {start_name} {relationship} {end_name}\n"

        # æ„å»ºå¢å¼ºçš„æç¤º
        enhanced_prompt = f"""åŸºäºä»¥ä¸‹çŸ¥è¯†ç”¨ä¸­æ–‡å›ç­”é—®é¢˜:
{knowledge_context}

ç”¨æˆ·é—®é¢˜: {message}

è¯·æä¾›å‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›ç­”:"""

        # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”
        response = services['llm'].generate_response(enhanced_prompt)

        return response

    except Exception as e:
        logger.error(f"çŸ¥è¯†å›¾è°±å¢å¼ºå¯¹è¯å¤±è´¥: {e}")
        # é™çº§åˆ°æ™®é€šå¯¹è¯
        return services['llm'].generate_response(message)

# ======================== æ¨¡å‹å‘ç°API ========================

@app.route('/api/discover_models', methods=['GET'])
def discover_models():
    """å‘ç°å¯ç”¨çš„æ¨¡å‹"""
    try:
        models_dir = 'models'
        available_models = []

        # æ£€æŸ¥modelsç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(models_dir):
            os.makedirs(models_dir, exist_ok=True)
            logger.info("åˆ›å»ºäº†modelsç›®å½•")

        # éå†modelsç›®å½•
        for item in os.listdir(models_dir):
            item_path = os.path.join(models_dir, item)

            # åªå¤„ç†æ–‡ä»¶å¤¹
            if os.path.isdir(item_path):
                model_info = {
                    "name": item,
                    "path": item_path,
                    "display_name": item,
                    "valid": False,
                    "size": 0,
                    "files": []
                }

                # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
                try:
                    model_files = []
                    total_size = 0

                    for root, dirs, files in os.walk(item_path):
                        for file in files:
                            file_path = os.path.join(root, file)
                            file_size = os.path.getsize(file_path)
                            total_size += file_size

                            # æ£€æŸ¥å…³é”®æ¨¡å‹æ–‡ä»¶
                            if file.endswith(('.bin', '.safetensors', '.pt', '.pth', '.ckpt')):
                                model_files.append({
                                    "name": file,
                                    "size": file_size,
                                    "type": "model"
                                })
                            elif file in ['config.json', 'tokenizer.json', 'tokenizer_config.json']:
                                model_files.append({
                                    "name": file,
                                    "size": file_size,
                                    "type": "config"
                                })

                    # åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆæ¨¡å‹
                    has_model_file = any(f['type'] == 'model' for f in model_files)
                    has_config = any(f['type'] == 'config' for f in model_files)

                    if has_model_file or has_config:
                        model_info["valid"] = True

                    model_info["size"] = total_size
                    model_info["files"] = model_files

                    # ç”Ÿæˆæ˜¾ç¤ºåç§°
                    if "deepseek" in item.lower():
                        model_info["display_name"] = f"ğŸ§  DeepSeek - {item}"
                    else:
                        model_info["display_name"] = f"ğŸ¤– {item}"

                    available_models.append(model_info)

                except Exception as e:
                    logger.warning(f"æ‰«ææ¨¡å‹ç›®å½• {item_path} æ—¶å‡ºé”™: {e}")
                    continue

        available_models.sort(key=lambda x: (not x["valid"], x["name"]))

        logger.info(f"å‘ç° {len(available_models)} ä¸ªæ¨¡å‹ç›®å½•")

        return jsonify({
            "status": "success",
            "models": available_models,
            "count": len(available_models)
        })

    except Exception as e:
        logger.error(f"æ¨¡å‹å‘ç°å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })

@app.route('/api/get_model_info', methods=['POST'])
def get_model_info():
    """è·å–ç‰¹å®šæ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        data = request.json
        model_path = data.get('model_path', '')

        if not model_path or not os.path.exists(model_path):
            return jsonify({
                "status": "error",
                "message": "æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨"
            })

        model_info = {
            "path": model_path,
            "name": os.path.basename(model_path),
            "files": [],
            "total_size": 0,
            "model_files_count": 0,
            "config_files": [],
            "tokenizer_files": []
        }

        # æ‰«ææ¨¡å‹æ–‡ä»¶
        for root, dirs, files in os.walk(model_path):
            for file in files:
                file_path = os.path.join(root, file)
                try:
                    file_size = os.path.getsize(file_path)
                    model_info["total_size"] += file_size

                    relative_path = os.path.relpath(file_path, model_path)

                    file_info = {
                        "name": file,
                        "path": relative_path,
                        "size": file_size,
                        "size_mb": round(file_size / (1024 * 1024), 2)
                    }

                    if file.endswith(('.bin', '.safetensors', '.pt', '.pth', '.ckpt')):
                        file_info["type"] = "model"
                        model_info["model_files_count"] += 1
                    elif file == 'config.json':
                        file_info["type"] = "config"
                        model_info["config_files"].append(file_info)

                        # å°è¯•è¯»å–é…ç½®ä¿¡æ¯
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                config = json.load(f)
                                model_info["model_type"] = config.get("model_type", "unknown")
                                model_info["architectures"] = config.get("architectures", [])
                                model_info["vocab_size"] = config.get("vocab_size", 0)
                        except:
                            pass
                    elif file in ['tokenizer.json', 'tokenizer_config.json', 'vocab.txt', 'merges.txt']:
                        file_info["type"] = "tokenizer"
                        model_info["tokenizer_files"].append(file_info)
                    else:
                        file_info["type"] = "other"

                    model_info["files"].append(file_info)

                except Exception as e:
                    logger.warning(f"æ— æ³•è·å–æ–‡ä»¶ä¿¡æ¯ {file_path}: {e}")
                    continue

        # è½¬æ¢æ€»å¤§å°ä¸ºå¯è¯»æ ¼å¼
        total_size_mb = model_info["total_size"] / (1024 * 1024)
        if total_size_mb > 1024:
            model_info["total_size_display"] = f"{total_size_mb / 1024:.1f} GB"
        else:
            model_info["total_size_display"] = f"{total_size_mb:.1f} MB"

        return jsonify({
            "status": "success",
            "model_info": model_info
        })

    except Exception as e:
        logger.error(f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })

@app.route('/api/check_quantization_support', methods=['GET'])
def check_quantization_support():
    """æ£€æŸ¥é‡åŒ–è®­ç»ƒæ”¯æŒæƒ…å†µ"""
    try:
        support_info = {
            "bitsandbytes_available": False,
            "accelerate_available": False,
            "peft_available": False,
            "torch_version": "",
            "cuda_available": False,
            "gpu_name": "",  # æ–°å¢ï¼šGPUåç§°
            "recommendations": []
        }

        # æ£€æŸ¥å¿…è¦çš„åº“
        try:
            import bitsandbytes
            support_info["bitsandbytes_available"] = True
            support_info["bitsandbytes_version"] = bitsandbytes.__version__
        except ImportError:
            support_info["recommendations"].append("éœ€è¦å®‰è£…bitsandbytes: pip install bitsandbytes")

        try:
            import accelerate
            support_info["accelerate_available"] = True
            support_info["accelerate_version"] = accelerate.__version__
        except ImportError:
            support_info["recommendations"].append("éœ€è¦å®‰è£…accelerate: pip install accelerate")

        try:
            import peft
            support_info["peft_available"] = True
            support_info["peft_version"] = peft.__version__
        except ImportError:
            support_info["recommendations"].append("éœ€è¦å®‰è£…peft: pip install peft")

        # æ£€æŸ¥PyTorchç‰ˆæœ¬
        try:
            import torch
            support_info["torch_version"] = torch.__version__
            support_info["cuda_available"] = torch.cuda.is_available()

            if torch.cuda.is_available():
                support_info["cuda_version"] = torch.version.cuda
                # è·å–GPUåç§°
                support_info["gpu_name"] = torch.cuda.get_device_name(0)
                logger.info(f"æ£€æµ‹åˆ°GPU: {support_info['gpu_name']}")
            else:
                support_info["gpu_name"] = "æ— GPU"

        except ImportError:
            support_info["recommendations"].append("éœ€è¦å®‰è£…PyTorch")

        # åˆ¤æ–­æ˜¯å¦æ”¯æŒé‡åŒ–è®­ç»ƒ
        quantization_ready = (
                support_info["bitsandbytes_available"] and
                support_info["accelerate_available"] and
                support_info["peft_available"]
        )

        support_info["quantization_ready"] = quantization_ready

        if not quantization_ready:
            support_info["recommendations"].append("å®‰è£…å‘½ä»¤: pip install bitsandbytes accelerate peft")

        return jsonify({
            "status": "success",
            "support_info": support_info
        })

    except Exception as e:
        logger.error(f"æ£€æŸ¥é‡åŒ–æ”¯æŒå¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })

# ======================== ç³»ç»ŸAPI ========================

@app.route('/api/system_status', methods=['GET'])
def get_system_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    return jsonify({
        "status": "success",
        "system_status": system_status,
        "services": {
            "llm": services['llm'] is not None,
            "fine_tuner": services['fine_tuner'] is not None,
            "knowledge_graph": services['knowledge_graph'] is not None,
            "teaching": services['teaching'] is not None,
            "speech_recognizer": services['speech_recognizer'] is not None,
            "image_recognizer": services['image_recognizer'] is not None,
            "text_processor": services['text_processor'] is not None
        }
    })

@app.route('/')
def index():
    """ä¸»é¡µ"""
    try:
        template_path = os.path.join('interface', 'web_console', 'templates', 'index.html')
        if os.path.exists(template_path):
            with open(template_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            return html_content
        else:
            return """
            <h1>æ™ºèƒ½æ•™å­¦ç³»ç»Ÿ</h1>
            <p>APIæœåŠ¡å™¨æ­£åœ¨è¿è¡Œ</p>
            <p>è¯·è®¿é—® <a href="/api/system_status">/api/system_status</a> æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€</p>
            """
    except Exception as e:
        logger.error(f"ä¸»é¡µè·¯ç”±å‡ºé”™: {e}")
        return f"<h1>ç³»ç»Ÿé”™è¯¯</h1><p>{str(e)}</p>"

# é”™è¯¯å¤„ç†
@app.errorhandler(404)
def not_found(error):
    return jsonify({
        "status": "error",
        "message": "APIæ¥å£ä¸å­˜åœ¨"
    }), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({
        "status": "error",
        "message": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"
    }), 500

if __name__ == '__main__':
    # åˆå§‹åŒ–æœåŠ¡
    initialize_services()

    logger.info("æ™ºèƒ½æ•™å­¦ç³»ç»ŸAPIæœåŠ¡å™¨å¯åŠ¨ä¸­...")
    logger.info("è®¿é—®åœ°å€: http://localhost:5000")

    # å¯åŠ¨Flaskåº”ç”¨
    app.run(host='0.0.0.0', port=5000, debug=True)