#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
PEPPERæ™ºèƒ½æ•™å­¦ç³»ç»Ÿ - å¢å¼ºAPIæœåŠ¡å™¨
æ”¯æŒå¤§è¯­è¨€æ¨¡å‹ã€çŸ¥è¯†å›¾è°±ã€å¤šæ¨¡æ€äº¤äº’ã€æ™ºèƒ½æ•™å­¦å››å¤§åŠŸèƒ½æ¨¡å—
"""
import gc
import json
import logging
import os
import sys
from datetime import datetime

import torch
from flask import Flask, request, jsonify
from flask_cors import CORS
from werkzeug.utils import secure_filename

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

# å¯¼å…¥å„ä¸ªæ¨¡å—
from ai_service.llm_module.llm_interface import LLMService
from ai_service.llm_module.lora_fine_tuning import LoRAFineTuner
from ai_service.knowledge_graph.knowledge_graph import KnowledgeGraph
from ai_service.knowledge_graph.education_knowledge_processor import EducationKnowledgeProcessor
from ai_service.multimodal.speech_recognition import SpeechRecognizer
from ai_service.multimodal.image_recognition import ImageRecognizer
from ai_service.multimodal.text_processor import TextProcessor

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("PEPPER_API")

app = Flask(__name__)
CORS(app)

# å…¨å±€å˜é‡å­˜å‚¨æœåŠ¡å®ä¾‹
services = {
    'llm': None,
    'fine_tuner': None,
    'knowledge_graph': None,
    'knowledge_processor': None,
    'speech_recognizer': None,
    'image_recognizer': None,
    'text_processor': None,
    'teaching': None
}

# ç³»ç»ŸçŠ¶æ€
system_status = {
    'model_loaded': False,
    'model_path': None,
    'model_quantization': None,
    'neo4j_connected': False,
    'training_progress': 0,
    'training_active': False,
    'training_start_time': None,
    'training_end_time': None,
    'training_output_dir': None,
    'training_config': None,
    'training_error': None,
    'auto_cleanup_enabled': True  # æ–°å¢ï¼šè‡ªåŠ¨æ¸…ç†å¼€å…³
}

# é…ç½®
UPLOAD_FOLDER = 'uploads'
REPORTS_FOLDER = 'reports'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(REPORTS_FOLDER, exist_ok=True)

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size


def force_cleanup_model(model_service=None):
    """å¼ºåˆ¶æ¸…ç†æ¨¡å‹å’Œé‡Šæ”¾å†…å­˜"""
    try:
        if model_service:
            # æ¸…ç†æŒ‡å®šæ¨¡å‹
            if hasattr(model_service, 'model') and model_service.model:
                del model_service.model
            if hasattr(model_service, 'peft_model') and model_service.peft_model:
                del model_service.peft_model
            if hasattr(model_service, 'tokenizer') and model_service.tokenizer:
                del model_service.tokenizer

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()

        # æ¸…ç†CUDAç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

        logger.info("æ¨¡å‹å†…å­˜æ¸…ç†å®Œæˆ")
        return True

    except Exception as e:
        logger.error(f"å¼ºåˆ¶æ¸…ç†æ¨¡å‹å¤±è´¥: {e}")
        return False


def initialize_services():
    """åˆå§‹åŒ–æœåŠ¡"""
    try:
        # åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨
        services['text_processor'] = TextProcessor()

        # åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å™¨
        services['speech_recognizer'] = SpeechRecognizer()

        # åˆå§‹åŒ–å›¾åƒè¯†åˆ«å™¨
        services['image_recognizer'] = ImageRecognizer()

        logger.info("åŸºç¡€æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        logger.error(f"æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")


# ======================== å¤§è¯­è¨€æ¨¡å‹API ========================

@app.route('/api/load_model', methods=['POST'])
def load_model():
    """åŠ è½½å¤§è¯­è¨€æ¨¡å‹"""
    try:
        data = request.json
        # ä¿®å¤ï¼šç¡®ä¿dataä¸ä¸ºç©º
        if not data:
            return jsonify({
                "status": "error",
                "message": "è¯·æ±‚æ•°æ®ä¸ºç©º"
            })

        model_path = data.get('model_path', '')
        use_4bit = data.get('use_4bit', True)  # é»˜è®¤ä½¿ç”¨4bité‡åŒ–
        use_8bit = data.get('use_8bit', False)

        # ä¿®å¤ï¼šæ£€æŸ¥model_pathæ˜¯å¦ä¸ºç©º
        if not model_path or model_path.strip() == '':
            return jsonify({
                "status": "error",
                "message": "æ¨¡å‹è·¯å¾„ä¸èƒ½ä¸ºç©º"
            })

        logger.info(f"æ­£åœ¨åŠ è½½æ¨ç†æ¨¡å‹: {model_path}")

        if not os.path.exists(model_path):
            return jsonify({
                "status": "error",
                "message": f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}"
            })

        # å¦‚æœæœ‰è®­ç»ƒä¸­çš„æ¨¡å‹ï¼Œå…ˆæ¸…ç†
        if services['fine_tuner']:
            logger.info("æ¸…ç†è®­ç»ƒæ¨¡å‹ï¼Œä¸ºæ¨ç†æ¨¡å‹è®©å‡ºå†…å­˜")
            force_cleanup_model(services['fine_tuner'])
            services['fine_tuner'] = None

        try:
            # æ£€æµ‹æ˜¯å¦æ˜¯å¾®è°ƒåçš„æ¨¡å‹
            if 'deepseek-' in model_path and any(
                    x in model_path for x in ['_4bit', '_8bit', '20241', '20242', '20243']):
                logger.info("æ£€æµ‹åˆ°å¾®è°ƒæ¨¡å‹ï¼Œä½¿ç”¨LoRAåŠ è½½")
                temp_fine_tuner = LoRAFineTuner(model_path)
                success = temp_fine_tuner.load_base_model(use_4bit=use_4bit, use_8bit=use_8bit)
                if success:
                    services['llm'] = temp_fine_tuner
            else:
                # åŸå§‹æ¨¡å‹ï¼Œä½¿ç”¨LLMService
                logger.info("æ£€æµ‹åˆ°åŸå§‹æ¨¡å‹ï¼Œä½¿ç”¨æ ‡å‡†åŠ è½½")
                services['llm'] = LLMService(model_path)
                success = True

        except Exception as e:
            logger.error(f"æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            return jsonify({
                "status": "error",
                "message": f"æ¨¡å‹åˆ›å»ºå¤±è´¥: {str(e)}"
            })

        if success:
            system_status['model_loaded'] = True
            system_status['model_path'] = model_path
            system_status['model_quantization'] = '4bit' if use_4bit else '8bit' if use_8bit else 'full'

            logger.info("æ¨ç†æ¨¡å‹åŠ è½½æˆåŠŸ")
            return jsonify({
                "status": "success",
                "message": "æ¨¡å‹åŠ è½½æˆåŠŸ",
                "model_path": model_path,
                "quantization": system_status['model_quantization']
            })
        else:
            return jsonify({
                "status": "error",
                "message": "æ¨¡å‹åŠ è½½å¤±è´¥"
            })

    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
        })


@app.route('/api/unload_model', methods=['POST'])
def unload_model():
    """å¸è½½å¤§è¯­è¨€æ¨¡å‹"""
    try:
        logger.info("å¼€å§‹å¸è½½æ¨¡å‹...")

        # æ¸…ç†æ¨ç†æ¨¡å‹
        if services['llm']:
            force_cleanup_model(services['llm'])
            services['llm'] = None

        # æ¸…ç†è®­ç»ƒæ¨¡å‹
        if services['fine_tuner']:
            force_cleanup_model(services['fine_tuner'])
            services['fine_tuner'] = None

        system_status['model_loaded'] = False
        system_status['model_path'] = None
        system_status['model_quantization'] = None

        logger.info("æ‰€æœ‰æ¨¡å‹å·²å¸è½½")
        return jsonify({
            "status": "success",
            "message": "æ¨¡å‹å·²å¸è½½"
        })

    except Exception as e:
        logger.error(f"æ¨¡å‹å¸è½½å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })


@app.route('/api/start_finetuning', methods=['POST'])
def start_finetuning():
    """å¼€å§‹LoRAå¾®è°ƒ"""
    try:
        if 'training_data' not in request.files:
            return jsonify({
                "status": "error",
                "message": "æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶"
            })

        file = request.files['training_data']
        model_path = request.form.get('model_path', '')
        epochs = int(request.form.get('epochs', 3))
        learning_rate = float(request.form.get('learning_rate', 0.0002))
        batch_size = int(request.form.get('batch_size', 2))  # 4bitè®­ç»ƒé»˜è®¤batch_size=2
        use_4bit = request.form.get('use_4bit', 'true').lower() == 'true'
        use_8bit = request.form.get('use_8bit', 'false').lower() == 'true'

        if file.filename == '':
            return jsonify({
                "status": "error",
                "message": "æœªé€‰æ‹©æ–‡ä»¶"
            })

        if not model_path or model_path.strip() == '':
            return jsonify({
                "status": "error",
                "message": "æœªæŒ‡å®šæ¨¡å‹è·¯å¾„"
            })

        # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            return jsonify({
                "status": "error",
                "message": f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}"
            })

        # æ¸…ç†ç°æœ‰çš„æ¨ç†æ¨¡å‹ï¼Œä¸ºè®­ç»ƒè®©å‡ºå†…å­˜
        if services['llm']:
            logger.info("æ¸…ç†æ¨ç†æ¨¡å‹ï¼Œä¸ºè®­ç»ƒè®©å‡ºå†…å­˜")
            force_cleanup_model(services['llm'])
            services['llm'] = None
            system_status['model_loaded'] = False

        # ä¿å­˜æ–‡ä»¶
        filename = secure_filename(file.filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(filepath)

        # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        quantization_suffix = "_4bit" if use_4bit else "_8bit" if use_8bit else ""
        output_dir = f"models/deepseek-{timestamp}{quantization_suffix}"

        # åˆ›å»ºå¾®è°ƒå™¨ï¼Œä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹è·¯å¾„
        services['fine_tuner'] = LoRAFineTuner(
            base_model_path=model_path,  # ä½¿ç”¨å‰ç«¯ä¼ é€’çš„æ¨¡å‹è·¯å¾„
            output_dir=output_dir
        )

        # è®°å½•è®­ç»ƒä¿¡æ¯åˆ°ç³»ç»ŸçŠ¶æ€
        system_status['training_output_dir'] = output_dir
        system_status['training_start_time'] = datetime.now().isoformat()
        system_status['training_active'] = True
        system_status['training_progress'] = 0
        system_status['training_config'] = {
            'model_path': model_path,
            'epochs': epochs,
            'batch_size': batch_size,
            'learning_rate': learning_rate,
            'use_4bit': use_4bit,
            'use_8bit': use_8bit,
            'quantization': '4bit' if use_4bit else '8bit' if use_8bit else 'none'
        }

        # å¯åŠ¨å¼‚æ­¥è®­ç»ƒä»»åŠ¡
        import threading
        def run_training():
            try:
                logger.info(f"å¼€å§‹{system_status['training_config']['quantization']}é‡åŒ–è®­ç»ƒ")

                # åŠ è½½åŸºç¡€æ¨¡å‹
                success = services['fine_tuner'].load_base_model(use_4bit=use_4bit, use_8bit=use_8bit)
                if not success:
                    system_status['training_active'] = False
                    system_status['training_error'] = "æ¨¡å‹åŠ è½½å¤±è´¥"
                    logger.error("è®­ç»ƒæ¨¡å‹åŠ è½½å¤±è´¥")
                    return

                system_status['training_progress'] = 5
                logger.info("è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‡†å¤‡LoRAé…ç½®")

                # å‡†å¤‡LoRAé…ç½®
                services['fine_tuner'].prepare_lora_config()
                system_status['training_progress'] = 10

                logger.info("å‡†å¤‡æ•°æ®é›†")
                # å‡†å¤‡æ•°æ®é›†
                dataset = services['fine_tuner'].prepare_dataset(data_path=filepath)
                if dataset is None:
                    system_status['training_active'] = False
                    system_status['training_error'] = "æ•°æ®é›†å‡†å¤‡å¤±è´¥"
                    logger.error("æ•°æ®é›†å‡†å¤‡å¤±è´¥")
                    return

                system_status['training_progress'] = 15
                logger.info("å¼€å§‹LoRAå¾®è°ƒè®­ç»ƒ")

                # å¼€å§‹è®­ç»ƒ
                success = services['fine_tuner'].train(
                    dataset,
                    epochs=epochs,
                    batch_size=batch_size,
                    learning_rate=learning_rate
                )

                if success:
                    system_status['training_progress'] = 100
                    system_status['training_end_time'] = datetime.now().isoformat()
                    logger.info(f"è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")

                    # è®­ç»ƒå®Œæˆåçš„è‡ªåŠ¨æ¸…ç†
                    logger.info("è®­ç»ƒå®Œæˆï¼Œå¼€å§‹è‡ªåŠ¨æ¸…ç†è®­ç»ƒæ¨¡å‹...")

                    # ç­‰å¾…ç¡®ä¿æ¨¡å‹ä¿å­˜å®Œæˆ
                    import time
                    time.sleep(3)  # å¢åŠ ç­‰å¾…æ—¶é—´åˆ°3ç§’

                    # æ‰§è¡Œæ¸…ç†
                    try:
                        if services['fine_tuner']:
                            force_cleanup_model(services['fine_tuner'])
                            services['fine_tuner'] = None
                            logger.info("âœ… è®­ç»ƒæ¨¡å‹å·²è‡ªåŠ¨æ¸…ç†ï¼Œå†…å­˜å·²é‡Šæ”¾")
                        else:
                            logger.info("è®­ç»ƒæ¨¡å‹å·²ç»ä¸ºç©ºï¼Œæ— éœ€æ¸…ç†")
                    except Exception as cleanup_error:
                        logger.error(f"è‡ªåŠ¨æ¸…ç†å¤±è´¥: {cleanup_error}")

                else:
                    system_status['training_error'] = "è®­ç»ƒè¿‡ç¨‹å¤±è´¥"
                    logger.error("è®­ç»ƒè¿‡ç¨‹å¤±è´¥")

            except Exception as e:
                logger.error(f"è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
                system_status['training_error'] = str(e)
            finally:
                system_status['training_active'] = False
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.remove(filepath)
                    logger.info("è®­ç»ƒæ•°æ®ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
                except:
                    pass

                # æœ€ç»ˆæ¸…ç†æ£€æŸ¥
                try:
                    if services['fine_tuner'] is not None:
                        logger.info("æ‰§è¡Œæœ€ç»ˆä¿é™©æ¸…ç†...")
                        force_cleanup_model(services['fine_tuner'])
                        services['fine_tuner'] = None
                        logger.info("æœ€ç»ˆæ¸…ç†å®Œæˆ")
                except Exception as final_cleanup_error:
                    logger.error(f"æœ€ç»ˆæ¸…ç†å¤±è´¥: {final_cleanup_error}")

        # å¯åŠ¨è®­ç»ƒçº¿ç¨‹
        thread = threading.Thread(target=run_training)
        thread.daemon = True
        thread.start()


        logger.info(f"LoRAå¾®è°ƒå·²å¼€å§‹ï¼Œè¾“å‡ºç›®å½•: {output_dir}")
        return jsonify({
            "status": "success",
            "message": "å¾®è°ƒå·²å¼€å§‹",
            "output_dir": output_dir,
            "timestamp": timestamp,
            "config": system_status['training_config']
        })

    except Exception as e:
        logger.error(f"å¾®è°ƒå¯åŠ¨å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })


@app.route('/api/training_progress', methods=['GET'])
def get_training_progress():
    """è·å–è®­ç»ƒè¿›åº¦"""
    progress_info = {
        "status": "success",
        "progress": system_status['training_progress'],
        "active": system_status['training_active'],
        "output_dir": system_status.get('training_output_dir', ''),
        "start_time": system_status.get('training_start_time', ''),
        "config": system_status.get('training_config', {}),
        "auto_cleanup_enabled": system_status.get('auto_cleanup_enabled', True)
    }

    # æ·»åŠ ç»“æŸæ—¶é—´ï¼ˆå¦‚æœæœ‰ï¼‰
    if 'training_end_time' in system_status:
        progress_info["end_time"] = system_status['training_end_time']

    # æ·»åŠ é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    if 'training_error' in system_status:
        progress_info["error"] = system_status['training_error']

    # è®¡ç®—é¢„ä¼°å‰©ä½™æ—¶é—´
    if system_status['training_active'] and 'training_start_time' in system_status:
        try:
            start_time = datetime.fromisoformat(system_status['training_start_time'])
            elapsed_time = (datetime.now() - start_time).total_seconds()
            progress = system_status['training_progress']

            if progress > 5:  # é¿å…é™¤é›¶é”™è¯¯
                estimated_total_time = elapsed_time * 100 / progress
                remaining_time = estimated_total_time - elapsed_time
                progress_info["estimated_remaining_seconds"] = max(0, int(remaining_time))
        except:
            pass

    # æ·»åŠ å†…å­˜ä½¿ç”¨ä¿¡æ¯
    try:
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3
            gpu_allocated = torch.cuda.memory_allocated() / 1024 ** 3
            gpu_cached = torch.cuda.memory_reserved() / 1024 ** 3

            progress_info["memory_info"] = {
                "gpu_total_gb": round(gpu_memory, 2),
                "gpu_allocated_gb": round(gpu_allocated, 2),
                "gpu_cached_gb": round(gpu_cached, 2),
                "gpu_free_gb": round(gpu_memory - gpu_cached, 2)
            }
    except:
        pass

    return jsonify(progress_info)


@app.route('/api/toggle_auto_cleanup', methods=['POST'])
def toggle_auto_cleanup():
    """åˆ‡æ¢è®­ç»ƒå®Œæˆåçš„è‡ªåŠ¨æ¸…ç†åŠŸèƒ½"""
    try:
        data = request.json
        enabled = data.get('enabled', True)

        system_status['auto_cleanup_enabled'] = enabled

        logger.info(f"è‡ªåŠ¨æ¸…ç†åŠŸèƒ½å·²{'å¯ç”¨' if enabled else 'ç¦ç”¨'}")

        return jsonify({
            "status": "success",
            "message": f"è‡ªåŠ¨æ¸…ç†åŠŸèƒ½å·²{'å¯ç”¨' if enabled else 'ç¦ç”¨'}",
            "auto_cleanup_enabled": enabled
        })

    except Exception as e:
        logger.error(f"åˆ‡æ¢è‡ªåŠ¨æ¸…ç†åŠŸèƒ½å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })


@app.route('/api/manual_cleanup', methods=['POST'])
def manual_cleanup():
    """æ‰‹åŠ¨æ¸…ç†æ¨¡å‹å†…å­˜"""
    try:
        cleanup_results = []

        # æ¸…ç†æ¨ç†æ¨¡å‹
        if services['llm']:
            force_cleanup_model(services['llm'])
            services['llm'] = None
            system_status['model_loaded'] = False
            cleanup_results.append("æ¨ç†æ¨¡å‹å·²æ¸…ç†")

        # æ¸…ç†è®­ç»ƒæ¨¡å‹
        if services['fine_tuner']:
            force_cleanup_model(services['fine_tuner'])
            services['fine_tuner'] = None
            cleanup_results.append("è®­ç»ƒæ¨¡å‹å·²æ¸…ç†")

        if not cleanup_results:
            cleanup_results.append("æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ¨¡å‹")

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            cleanup_results.append("GPUç¼“å­˜å·²æ¸…ç†")

        logger.info(f"æ‰‹åŠ¨æ¸…ç†å®Œæˆ: {', '.join(cleanup_results)}")

        return jsonify({
            "status": "success",
            "message": "æ‰‹åŠ¨æ¸…ç†å®Œæˆ",
            "details": cleanup_results
        })

    except Exception as e:
        logger.error(f"æ‰‹åŠ¨æ¸…ç†å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })


@app.route('/api/system_memory_info', methods=['GET'])
def get_system_memory_info():
    """è·å–ç³»ç»Ÿå†…å­˜ä½¿ç”¨ä¿¡æ¯"""
    try:
        import psutil

        memory_info = {
            "cpu_memory": {
                "total_gb": round(psutil.virtual_memory().total / 1024 ** 3, 2),
                "available_gb": round(psutil.virtual_memory().available / 1024 ** 3, 2),
                "used_gb": round(psutil.virtual_memory().used / 1024 ** 3, 2),
                "percent": psutil.virtual_memory().percent
            }
        }

        # GPUå†…å­˜ä¿¡æ¯
        if torch.cuda.is_available():
            memory_info["gpu_memory"] = {
                "total_gb": round(torch.cuda.get_device_properties(0).total_memory / 1024 ** 3, 2),
                "allocated_gb": round(torch.cuda.memory_allocated() / 1024 ** 3, 2),
                "cached_gb": round(torch.cuda.memory_reserved() / 1024 ** 3, 2),
                "free_gb": round(
                    (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1024 ** 3, 2)
            }
        else:
            memory_info["gpu_memory"] = {"available": False}

        # æ¨¡å‹çŠ¶æ€
        memory_info["model_status"] = {
            "inference_model_loaded": services['llm'] is not None,
            "training_model_loaded": services['fine_tuner'] is not None,
            "training_active": system_status['training_active']
        }

        return jsonify({
            "status": "success",
            "memory_info": memory_info
        })

    except Exception as e:
        logger.error(f"è·å–å†…å­˜ä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })


# ======================== å…¶ä»–APIä¿æŒä¸å˜ ========================

@app.route('/api/test_model', methods=['POST'])
def test_model():
    """æµ‹è¯•å¤§è¯­è¨€æ¨¡å‹"""
    try:
        if not services['llm']:
            return jsonify({
                "status": "error",
                "message": "æ¨¡å‹æœªåŠ è½½"
            })

        data = request.json
        question = data.get('question', '')
        max_tokens = data.get('max_tokens', 512)

        if not question.strip():
            return jsonify({
                "status": "error",
                "message": "é—®é¢˜ä¸èƒ½ä¸ºç©º"
            })

        # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”
        if hasattr(services['llm'], 'generate_response'):
            response = services['llm'].generate_response(question, max_tokens)
        else:
            response = services['llm'].generate_response(question, max_tokens)

        return jsonify({
            "status": "success",
            "response": response,
            "model_quantization": system_status.get('model_quantization', 'unknown')
        })

    except Exception as e:
        logger.error(f"æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })


# ======================== çŸ¥è¯†å›¾è°±API ========================

@app.route('/api/connect_neo4j', methods=['POST'])
def connect_neo4j():
    """è¿æ¥Neo4jæ•°æ®åº“"""
    try:
        data = request.json
        uri = data.get('uri', 'bolt://localhost:7687')
        user = data.get('user', 'neo4j')
        password = data.get('password', 'password')

        services['knowledge_graph'] = KnowledgeGraph(uri, user, password)
        services['knowledge_processor'] = EducationKnowledgeProcessor(uri, user, password)

        # æµ‹è¯•è¿æ¥
        test_result = services['knowledge_graph'].query("RETURN 1 as test")
        if test_result:
            system_status['neo4j_connected'] = True
            logger.info("Neo4jè¿æ¥æˆåŠŸ")
            return jsonify({
                "status": "success",
                "message": "æ•°æ®åº“è¿æ¥æˆåŠŸ"
            })
        else:
            raise Exception("è¿æ¥æµ‹è¯•å¤±è´¥")

    except Exception as e:
        logger.error(f"Neo4jè¿æ¥å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })


@app.route('/api/generate_sample_knowledge', methods=['POST'])
def generate_sample_knowledge():
    """ç”Ÿæˆç¤ºä¾‹çŸ¥è¯†"""
    try:
        if not services['knowledge_processor']:
            return jsonify({
                "status": "error",
                "message": "æœªè¿æ¥çŸ¥è¯†å›¾è°±æ•°æ®åº“"
            })

        count = services['knowledge_processor'].create_educational_knowledge_base()

        logger.info("ç¤ºä¾‹çŸ¥è¯†ç”ŸæˆæˆåŠŸ")
        return jsonify({
            "status": "success",
            "message": "ç¤ºä¾‹çŸ¥è¯†ç”ŸæˆæˆåŠŸ",
            "count": count
        })

    except Exception as e:
        logger.error(f"ç¤ºä¾‹çŸ¥è¯†ç”Ÿæˆå¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })


@app.route('/api/chat', methods=['POST'])
def chat():
    """æ™ºèƒ½å¯¹è¯"""
    try:
        data = request.json
        message = data.get('message', '')

        if not message.strip():
            return jsonify({
                "status": "error",
                "message": "æ¶ˆæ¯ä¸èƒ½ä¸ºç©º"
            })

        # å¦‚æœæœ‰æ¨¡å‹ï¼Œä½¿ç”¨æ¨¡å‹å›ç­”
        if services['llm']:
            response = services['llm'].generate_response(message)
        else:
            response = "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚è¯·ç¡®ä¿å·²åŠ è½½è¯­è¨€æ¨¡å‹ã€‚"

        return jsonify({
            "status": "success",
            "response": response
        })

    except Exception as e:
        logger.error(f"æ™ºèƒ½å¯¹è¯å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })


# ======================== æ¨¡å‹å‘ç°API ========================

@app.route('/api/discover_models', methods=['GET'])
def discover_models():
    """å‘ç°å¯ç”¨çš„æ¨¡å‹"""
    try:
        models_dir = 'models'
        available_models = []

        # æ£€æŸ¥modelsç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(models_dir):
            os.makedirs(models_dir, exist_ok=True)
            logger.info("åˆ›å»ºäº†modelsç›®å½•")

        # éå†modelsç›®å½•
        for item in os.listdir(models_dir):
            item_path = os.path.join(models_dir, item)

            # åªå¤„ç†æ–‡ä»¶å¤¹
            if os.path.isdir(item_path):
                model_info = {
                    "name": item,
                    "path": item_path,
                    "display_name": item,
                    "valid": False,
                    "size": 0,
                    "files": []
                }

                # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
                try:
                    model_files = []
                    total_size = 0

                    for root, dirs, files in os.walk(item_path):
                        for file in files:
                            file_path = os.path.join(root, file)
                            file_size = os.path.getsize(file_path)
                            total_size += file_size

                            # æ£€æŸ¥å…³é”®æ¨¡å‹æ–‡ä»¶
                            if file.endswith(('.bin', '.safetensors', '.pt', '.pth', '.ckpt')):
                                model_files.append({
                                    "name": file,
                                    "size": file_size,
                                    "type": "model"
                                })
                            elif file in ['config.json', 'tokenizer.json', 'tokenizer_config.json']:
                                model_files.append({
                                    "name": file,
                                    "size": file_size,
                                    "type": "config"
                                })

                    # åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆæ¨¡å‹
                    has_model_file = any(f['type'] == 'model' for f in model_files)
                    has_config = any(f['type'] == 'config' for f in model_files)

                    if has_model_file or has_config:
                        model_info["valid"] = True

                    model_info["size"] = total_size
                    model_info["files"] = model_files

                    # ç”Ÿæˆæ˜¾ç¤ºåç§°
                    if "deepseek" in item.lower():
                        model_info["display_name"] = f"ğŸ§  DeepSeek - {item}"
                    else:
                        model_info["display_name"] = f"ğŸ¤– {item}"

                    available_models.append(model_info)

                except Exception as e:
                    logger.warning(f"æ‰«ææ¨¡å‹ç›®å½• {item_path} æ—¶å‡ºé”™: {e}")
                    continue

        available_models.sort(key=lambda x: (not x["valid"], x["name"]))

        logger.info(f"å‘ç° {len(available_models)} ä¸ªæ¨¡å‹ç›®å½•")

        return jsonify({
            "status": "success",
            "models": available_models,
            "count": len(available_models)
        })

    except Exception as e:
        logger.error(f"æ¨¡å‹å‘ç°å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })

@app.route('/api/get_model_info', methods=['POST'])
def get_model_info():
    """è·å–ç‰¹å®šæ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        data = request.json
        model_path = data.get('model_path', '')

        if not model_path or not os.path.exists(model_path):
            return jsonify({
                "status": "error",
                "message": "æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨"
            })

        model_info = {
            "path": model_path,
            "name": os.path.basename(model_path),
            "files": [],
            "total_size": 0,
            "model_files_count": 0,
            "config_files": [],
            "tokenizer_files": []
        }

        # æ‰«ææ¨¡å‹æ–‡ä»¶
        for root, dirs, files in os.walk(model_path):
            for file in files:
                file_path = os.path.join(root, file)
                try:
                    file_size = os.path.getsize(file_path)
                    model_info["total_size"] += file_size

                    relative_path = os.path.relpath(file_path, model_path)

                    file_info = {
                        "name": file,
                        "path": relative_path,
                        "size": file_size,
                        "size_mb": round(file_size / (1024 * 1024), 2)
                    }

                    if file.endswith(('.bin', '.safetensors', '.pt', '.pth', '.ckpt')):
                        file_info["type"] = "model"
                        model_info["model_files_count"] += 1
                    elif file == 'config.json':
                        file_info["type"] = "config"
                        model_info["config_files"].append(file_info)

                        # å°è¯•è¯»å–é…ç½®ä¿¡æ¯
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                config = json.load(f)
                                model_info["model_type"] = config.get("model_type", "unknown")
                                model_info["architectures"] = config.get("architectures", [])
                                model_info["vocab_size"] = config.get("vocab_size", 0)
                        except:
                            pass
                    elif file in ['tokenizer.json', 'tokenizer_config.json', 'vocab.txt', 'merges.txt']:
                        file_info["type"] = "tokenizer"
                        model_info["tokenizer_files"].append(file_info)
                    else:
                        file_info["type"] = "other"

                    model_info["files"].append(file_info)

                except Exception as e:
                    logger.warning(f"æ— æ³•è·å–æ–‡ä»¶ä¿¡æ¯ {file_path}: {e}")
                    continue

        # è½¬æ¢æ€»å¤§å°ä¸ºå¯è¯»æ ¼å¼
        total_size_mb = model_info["total_size"] / (1024 * 1024)
        if total_size_mb > 1024:
            model_info["total_size_display"] = f"{total_size_mb / 1024:.1f} GB"
        else:
            model_info["total_size_display"] = f"{total_size_mb:.1f} MB"

        return jsonify({
            "status": "success",
            "model_info": model_info
        })

    except Exception as e:
        logger.error(f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })


@app.route('/api/check_quantization_support', methods=['GET'])
def check_quantization_support():
    """æ£€æŸ¥é‡åŒ–è®­ç»ƒæ”¯æŒæƒ…å†µ"""
    try:
        support_info = {
            "bitsandbytes_available": False,
            "accelerate_available": False,
            "peft_available": False,
            "torch_version": "",
            "cuda_available": False,
            "recommendations": []
        }

        # æ£€æŸ¥å¿…è¦çš„åº“
        try:
            import bitsandbytes
            support_info["bitsandbytes_available"] = True
            support_info["bitsandbytes_version"] = bitsandbytes.__version__
        except ImportError:
            support_info["recommendations"].append("éœ€è¦å®‰è£…bitsandbytes: pip install bitsandbytes")

        try:
            import accelerate
            support_info["accelerate_available"] = True
            support_info["accelerate_version"] = accelerate.__version__
        except ImportError:
            support_info["recommendations"].append("éœ€è¦å®‰è£…accelerate: pip install accelerate")

        try:
            import peft
            support_info["peft_available"] = True
            support_info["peft_version"] = peft.__version__
        except ImportError:
            support_info["recommendations"].append("éœ€è¦å®‰è£…peft: pip install peft")

        # æ£€æŸ¥PyTorchç‰ˆæœ¬
        try:
            import torch
            support_info["torch_version"] = torch.__version__
            support_info["cuda_available"] = torch.cuda.is_available()
            if torch.cuda.is_available():
                support_info["cuda_version"] = torch.version.cuda
        except ImportError:
            support_info["recommendations"].append("éœ€è¦å®‰è£…PyTorch")

        # åˆ¤æ–­æ˜¯å¦æ”¯æŒé‡åŒ–è®­ç»ƒ
        quantization_ready = (
                support_info["bitsandbytes_available"] and
                support_info["accelerate_available"] and
                support_info["peft_available"]
        )

        support_info["quantization_ready"] = quantization_ready

        if not quantization_ready:
            support_info["recommendations"].append("å®‰è£…å‘½ä»¤: pip install bitsandbytes accelerate peft")

        return jsonify({
            "status": "success",
            "support_info": support_info
        })

    except Exception as e:
        logger.error(f"æ£€æŸ¥é‡åŒ–æ”¯æŒå¤±è´¥: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        })


# ======================== ç³»ç»ŸAPI ========================

@app.route('/api/system_status', methods=['GET'])
def get_system_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    return jsonify({
        "status": "success",
        "system_status": system_status,
        "services": {
            "llm": services['llm'] is not None,
            "fine_tuner": services['fine_tuner'] is not None,
            "knowledge_graph": services['knowledge_graph'] is not None,
            "teaching": services['teaching'] is not None,
            "speech_recognizer": services['speech_recognizer'] is not None,
            "image_recognizer": services['image_recognizer'] is not None,
            "text_processor": services['text_processor'] is not None
        }
    })


@app.route('/')
def index():
    """ä¸»é¡µ"""
    try:
        template_path = os.path.join('interface', 'web_console', 'templates', 'index.html')
        if os.path.exists(template_path):
            with open(template_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            return html_content
        else:
            return """
            <h1>PEPPERæ™ºèƒ½æ•™å­¦ç³»ç»Ÿ</h1>
            <p>APIæœåŠ¡å™¨æ­£åœ¨è¿è¡Œ</p>
            <p>è¯·è®¿é—® <a href="/api/system_status">/api/system_status</a> æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€</p>
            """
    except Exception as e:
        logger.error(f"ä¸»é¡µè·¯ç”±å‡ºé”™: {e}")
        return f"<h1>ç³»ç»Ÿé”™è¯¯</h1><p>{str(e)}</p>"


# é”™è¯¯å¤„ç†
@app.errorhandler(404)
def not_found(error):
    return jsonify({
        "status": "error",
        "message": "APIæ¥å£ä¸å­˜åœ¨"
    }), 404


@app.errorhandler(500)
def internal_error(error):
    return jsonify({
        "status": "error",
        "message": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"
    }), 500


if __name__ == '__main__':
    # åˆå§‹åŒ–æœåŠ¡
    initialize_services()

    logger.info("PEPPERæ™ºèƒ½æ•™å­¦ç³»ç»ŸAPIæœåŠ¡å™¨å¯åŠ¨ä¸­...")
    logger.info("è®¿é—®åœ°å€: http://localhost:5000")

    # å¯åŠ¨Flaskåº”ç”¨
    app.run(host='0.0.0.0', port=5000, debug=True)